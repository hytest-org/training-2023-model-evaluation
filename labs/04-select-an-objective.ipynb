{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6482af7-ea2f-46a8-9ba3-2d9c7cfd2f91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select an objective\n",
    "\n",
    "## Load data\n",
    "First load a dataset containing some toy model predictions. Here we'll compare NNDAR against streamgage observations (obs). By nature, NNDAR predictions are 'out of sample'; otherwise, we'd need to use cross validation or information criteria to estimate the out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4c456-4325-495e-9d08-ab7d10fceb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data from s3 (run once)\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fs_read = fsspec.filesystem(\n",
    "    \"s3\", anon=True, client_kwargs={\"endpoint_url\": \"https://renc.osn.xsede.org\"}\n",
    ")\n",
    "\n",
    "with fs_read.open(\"s3://rsignellbucket2/hytest/thodson/gages2_nndar.parquet\") as f:\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "# threshold low values\n",
    "df[df < 0.01] = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd8c5b-851b-46db-966e-b6c09176e0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-09T14:05:24.197225Z",
     "iopub.status.busy": "2023-05-09T14:05:24.196741Z",
     "iopub.status.idle": "2023-05-09T14:05:24.202648Z",
     "shell.execute_reply": "2023-05-09T14:05:24.201880Z",
     "shell.execute_reply.started": "2023-05-09T14:05:24.197200Z"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "The first step in model evaluation is to select an appropriate model for representing error (or uncertainty) in your model;\n",
    "in other words, select an objective function.\n",
    "\n",
    "For example, if the errors are normally distributed and /iid/, \n",
    "the optimal objective function is mean squared error. \n",
    "We'll review why shortly. \n",
    "However, for many models, the error distribution is neither normal nor iid.\n",
    "Runoff models being a pertinent example.\n",
    "Their errors tend to scale with the magnitude of flow (heteroscedasticity).\n",
    "Logging the data helps to mitigate that,\n",
    "which is what most hydrologists want,\n",
    "even if they don't do it in practice.\n",
    "\n",
    "\n",
    "The next sections demonstrate a basic procedure for selecting a reasonable objective function for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148f059-51b5-4316-b950-a0ad57e2ec78",
   "metadata": {},
   "source": [
    "## Benchmarking objectives\n",
    "Pretend that a model has a MSE of 1 and a MSLE of 1.\n",
    "Which is a better score?\n",
    "The short answer is we don't know.\n",
    "These scores have different scales, so we can't compare them directly.\n",
    "\n",
    "\n",
    "Fortunately, we already have everything we need.\n",
    "The trick is to transform the objectives into likelihoods,\n",
    "putting them all on a common scale.\n",
    "We'll demonstrate the simplest approach of using maximum likelihood estimators.\n",
    "\n",
    "### Log likelihoods\n",
    "The first, and arguably de facto, objective is MSE, \n",
    "which corresponds to the log likelihood of the normal distribution,\n",
    "\\begin{equation}\n",
    "\\ell_2 = -n \\ln \\sigma  - \\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\text{,}\n",
    "\\end{equation}\n",
    "where $y_i$ are the observations, $\\hat y_i$ are the model predictions, and $\\sigma$ is standard deviation of the error.\n",
    "The final term is the most important.\n",
    "Stare at it for a moment and you'll recognize it as the L2 norm,\n",
    "which is essentially just the MSE.\n",
    "The remaining terms normalize the result, which we need in order to compare other objective functions.\n",
    "If we calibrate a model to just MSE, we could drop these terms.\n",
    "However, to compare different objectives, we need to keep them.\n",
    "\n",
    "Another common objective is mean absolute error (MAE),\n",
    "which corresponds to the log likelihood of the Laplace distribution,\n",
    "\\begin{equation}\n",
    "\\ell_1 = -n \\ln(2b)  - \\frac{1}{b} \\sum_{i=1}^n | y_i - \\hat y_i| \\text{,}\n",
    "\\end{equation}\n",
    "where $b$ is mean absolute error\n",
    "(also known as the L1 norm).\n",
    "\n",
    "### Changing variables\n",
    "Likelihoods for a variety of other objective functions are obtained by changing variables.\n",
    "For example, the mean squared log error (MSLE), which corresponds to the lognormal log likelihood $\\ell_3$,\n",
    "is obtained from $\\ell_2$ by changing variables\n",
    "\\begin{equation}\n",
    "\\ell_3 = \\ell_2(v(y)) + \\ln|v'(y)| \\text{,}\n",
    "\\end{equation}\n",
    "where $v$, the natural log in this case.\n",
    "\n",
    "Can you define more?\n",
    "\n",
    "### Additional reading\n",
    "```\n",
    "@Article{Hodson_2022,\n",
    "  doi = {10.48550/ARXIV.2212.06566},\n",
    "  author = {Hodson, Timothy O. and Over, Thomas M. and Smith, Tyler J. and Marshall, Lucy M.},\n",
    "  title = {How to select an objective function using information theory},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab259b-2a54-452a-b973-1dbcc8efa4b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print one gage just to check that the data has loaded\n",
    "df.loc[(\"01013500\", ...)]\n",
    "gage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93e76d-12bb-4130-9a35-ec1a1d3568fb",
   "metadata": {},
   "source": [
    "Now define a function to return the log likelihood assuming the errors follow a normal distribution, which is equivalent to using MSE as an objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef5f69-426e-4ebe-a9f7-bcec144a0531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normal_ll(y, y_hat):\n",
    "    \"\"\"Compute log likelihood for the normal distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Log likelihood\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "normal_ll(gage[\"obs\"], gage[\"NNDAR\"])  # should return -121363.3792673729"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e4d46-9b44-4c0f-b2ea-c2dbec52420f",
   "metadata": {},
   "source": [
    "Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb58f9-abce-4095-b7cf-e89b8f2d2591",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normal_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for the normal distribution.\n",
    "\n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "\n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    return  -n * np.log(sigma) - n/2 * np.log(2 * np.pi) - 1 / (2 * sigma**2) * (e**2).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54f93c-f583-461e-96a7-9ebe8248d2c5",
   "metadata": {},
   "source": [
    "Now try the log likelihood of the lognormal distribution.\n",
    "\n",
    "Hint: Ideally use a change of variable, but you can also hard code each distribution. For the lognormal, see the PDF at https://en.wikipedia.org/wiki/Log-normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a6f59f-b19d-4273-9749-001cbbced3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lognormal_ll(y, y_hat):\n",
    "    \"\"\"Compute log likelihood for the lognormal distribution.\n",
    "    \n",
    "    Hint: Use a change of variables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Log likelihood\n",
    "\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# test\n",
    "lognormal_ll(gage[\"obs\"], gage[\"NNDAR\"])  # should return -108803.34768683679"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b326c-8234-41c0-8dbf-d54b095b6944",
   "metadata": {},
   "source": [
    "Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5607023-b656-4624-bd3a-8e0d0a9afd8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lognormal_ll(y, y_hat):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    log_gradient = np.sum(np.log(np.abs(1/y)))\n",
    "    ll = normal_ll(y, y_hat) + log_gradient\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea1218-7b07-40ea-8d8f-cd9616da6542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-08T22:16:34.827807Z",
     "iopub.status.busy": "2023-05-08T22:16:34.827445Z",
     "iopub.status.idle": "2023-05-08T22:16:34.830632Z",
     "shell.execute_reply": "2023-05-08T22:16:34.830083Z",
     "shell.execute_reply.started": "2023-05-08T22:16:34.827785Z"
    },
    "tags": []
   },
   "source": [
    "## More log likelihoods\n",
    "To make the benchmark interesting, we'll use the change of variable technique to derive log likelihoods for several other objective functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7e15f-01c7-4810-adab-7d8e2fc1af95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def normal_ll(y, y_hat, transform=None, gradient=1):\n",
    "    \"\"\"Log likelihood for the normal distribution with change of variable\n",
    "\n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "\n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = (\n",
    "        -n * np.log(sigma)\n",
    "        - n / 2 * np.log(2 * np.pi)\n",
    "        - 1 / (2 * sigma**2) * (e**2).sum()\n",
    "        + log_gradient\n",
    "    )\n",
    "    return ll\n",
    "\n",
    "\n",
    "def laplace_ll(y, y_hat, transform=None, gradient=1):\n",
    "    \"\"\"Log likelihood for Laplace distribution with change of variable\n",
    "\n",
    "    The laplace distribution is the formal likelihood for the mean absolute\n",
    "    error (MAE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "    \"\"\"\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "\n",
    "    e = (y - y_hat).abs()\n",
    "    n = len(e)\n",
    "    b = e.mean()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = -n * np.log(2 * b) - 1 / b * e.sum() + log_gradient\n",
    "    return ll.sum()\n",
    "\n",
    "\n",
    "def msre_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for mean squared square-root error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return normal_ll(\n",
    "        y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1 / (2 * np.sqrt(y))\n",
    "    )\n",
    "\n",
    "\n",
    "def mare_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for mean absolute square-root error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return laplace_ll(\n",
    "        y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1 / (2 * np.sqrt(y))\n",
    "    )\n",
    "\n",
    "\n",
    "def lognormal_ll(y, y_hat):\n",
    "    \"\"\"Lognormal log likelihood\n",
    "\n",
    "    The lognormal distribution is the formal likelihood for the mean squared\n",
    "    log error (MSLE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return normal_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1 / y)\n",
    "\n",
    "\n",
    "def mspe_ll(y, y_hat):\n",
    "    \"\"\"Log likelhood for mean squared percentage error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "\n",
    "    \"\"\"\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x / y, gradient=-1 / (y**2))\n",
    "\n",
    "\n",
    "def nse_ll(y, y_hat, group=\"gage_id\"):\n",
    "    \"\"\"Log likelihood for normalized squared error (NSE)\n",
    "\n",
    "    NSE is equivalent to the Nashâ€“Sutcliffe model efficiency coefficient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    sigma_o = y.groupby(\"gage_id\").transform(lambda x: x.std())\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x / sigma_o, gradient=1 / sigma_o)\n",
    "\n",
    "\n",
    "def loglaplace_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for log Laplace distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return laplace_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1 / y)\n",
    "\n",
    "\n",
    "def uniform_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for uniform distribution.\n",
    "\n",
    "    The uniform log likelihood minimizes the maximum error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    e = np.abs(y - y_hat)\n",
    "    n = len(e)\n",
    "    # ll = -n * np.log(e.max()-e.min()) # standard formulation\n",
    "    ll = -n * np.log(e.max() - 0)\n",
    "    return ll\n",
    "\n",
    "\n",
    "def bernoulli_ll(y, y_hat, groupby=None):\n",
    "    \"\"\"TODO and use within zi_ll\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def zi_ll(y, y_hat, ll=normal_ll, threshold=0.01, groupby=None):\n",
    "    \"\"\"Zero-inflated log likelihood.\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    ll : function\n",
    "        Zero-inflated log likelihood\n",
    "    threshold : float\n",
    "        Value below which is treated as zero\n",
    "    groupby : string\n",
    "        Optional groupby term (testing)\n",
    "    \"\"\"\n",
    "    y_o = y <= threshold\n",
    "    y_hat_o = y_hat <= threshold\n",
    "\n",
    "    if groupby is None:\n",
    "        n1 = (y_o & y_hat_o).sum()  # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).sum()  # incorrect zero-flow prediction\n",
    "    else:\n",
    "        n1 = (y_o & y_hat_o).groupby(groupby).sum()  # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).groupby(groupby).sum()  # incorrect zero-flow prediction\n",
    "\n",
    "    n3 = ~y_o & ~y_hat_o  # correct flow predictions\n",
    "\n",
    "    # fraction of correctly predicted zero flows\n",
    "    rho = np.where((n1 + n2) == 0, 0, n1 / (n1 + n2))\n",
    "    n_rho = 1 - rho\n",
    "\n",
    "    # n1 * np.log(rho) + n2 * np.log(1-rho)\n",
    "    ll_zero = n1[rho != 0] * np.log(rho[rho != 0]) + n2[n_rho != 0] * np.log(\n",
    "        n_rho[n_rho != 0]\n",
    "    )\n",
    "\n",
    "    return ll_zero.sum() + ll(y[n3], y_hat[n3])\n",
    "\n",
    "\n",
    "def zilognormal_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for zero-inflated lognormal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "\n",
    "    return zi_ll(y, y_hat, ll=lognormal_ll, threshold=0.01)\n",
    "\n",
    "\n",
    "def ziloglaplace_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for zero-inflated laplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return zi_ll(y, y_hat, ll=loglaplace_ll, threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d767c5-98f3-4322-8362-3734a7867bbd",
   "metadata": {},
   "source": [
    "## The experiment\n",
    "Like any model, to evaluate competing objective functions, we compare their log likelihoods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593b3f0-c33c-4458-9bfb-423da35e99d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 1: create a table of objective functions\n",
    "objectives = {\n",
    "    \"U\": {\"name\": \"uniformly distributed error\", \"f\": uniform_ll},\n",
    "    \"MSE\": {\"name\": \"mean squared error\", \"f\": normal_ll},\n",
    "    \"NSE\": {\"name\": \"normalized squared error\", \"f\": nse_ll},\n",
    "    \"MAE\": {\"name\": \"mean absolute error\", \"f\": laplace_ll},\n",
    "    \"MSPE\": {\"name\": \"mean squared percent error\", \"f\": mspe_ll},\n",
    "    \"MSLE\": {\"name\": \"mean squared log error*\", \"f\": lognormal_ll},\n",
    "    \"MALE\": {\"name\": \"mean absolute log error*\", \"f\": loglaplace_ll},\n",
    "    \"ZMSLE\": {\"name\": \"zero-inflated MSLE\", \"f\": zilognormal_ll},\n",
    "    \"ZMALE\": {\"name\": \"zero-inflated MALE\", \"f\": ziloglaplace_ll},\n",
    "    \"MARE\": {\"name\": \"mean absolute square root error\", \"f\": mare_ll},\n",
    "}\n",
    "\n",
    "obj_df = pd.DataFrame.from_dict(objectives, orient=\"index\")\n",
    "\n",
    "# step 2: compute the information in each objective function\n",
    "for index, row in obj_df.iterrows():\n",
    "    # nats is the negative log likelihood or the info in the error\n",
    "    obj_df.loc[index, \"ll\"] = row.f(df.obs, df.NNDAR)\n",
    "\n",
    "obj_df = obj_df[[\"name\", \"ll\"]]\n",
    "\n",
    "obj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ea2b3-b1a0-4802-b5ee-05340eda3e89",
   "metadata": {},
   "source": [
    "But raw log likelihoods can be difficult to digest, so we typically rescale them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b14e7-447c-4b42-b8e7-d9107bc623b2",
   "metadata": {},
   "source": [
    "## Akaike weights\n",
    "\n",
    "In practice, we aren't intersted in the magnitude of the log likelihood, only their differences.\n",
    "One approach is to represent each by its Akaike weights, which represents the probability that each objective is the correct on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b16ac8-609e-43c8-a116-3663646e64ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_weights(series, base=np.e):\n",
    "    \"\"\"Compute posterior weights\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array_like\n",
    "        Log likelihoods\n",
    "    base: float\n",
    "        Base of the logarithm used to compute log likelihood\n",
    "    \"\"\"\n",
    "    s = base**series\n",
    "    return s / s.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86bfe2-b0dc-40a6-a6d9-463d0d9baeea",
   "metadata": {},
   "source": [
    "## Bits\n",
    "\n",
    "We can also convert the log likelihood to bits. Their magnitude is still meaningless, but at least the scale is more digestable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b7cea5-72ac-4dd6-9a6d-b5ca8d97898c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_info(ll, n, base=2):\n",
    "    \"\"\"Convert a log likelihood to bits\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ll : array_like\n",
    "        Log likelihoods\n",
    "    n :\n",
    "    base: float\n",
    "        Base of the logarithm; 2 = bits\n",
    "    \"\"\"\n",
    "    return -ll / n / np.log(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176057b-979a-4cf5-801e-b8348ff98533",
   "metadata": {},
   "source": [
    "Now we're ready to generate some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987eeec-2a42-431f-9ed9-b2844be2b450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# step 1: create a table of objective functions\n",
    "objectives = {\n",
    "    \"U\": {\"name\": \"uniformly distributed error\", \"f\": uniform_ll},\n",
    "    \"MSE\": {\"name\": \"mean squared error\", \"f\": normal_ll},\n",
    "    \"NSE\": {\"name\": \"normalized squared error\", \"f\": nse_ll},\n",
    "    \"MAE\": {\"name\": \"mean absolute error\", \"f\": laplace_ll},\n",
    "    \"MSPE\": {\"name\": \"mean squared percent error\", \"f\": mspe_ll},\n",
    "    \"MSLE\": {\"name\": \"mean squared log error*\", \"f\": lognormal_ll},\n",
    "    \"MALE\": {\"name\": \"mean absolute log error*\", \"f\": loglaplace_ll},\n",
    "    \"ZMSLE\": {\"name\": \"zero-inflated MSLE\", \"f\": zilognormal_ll},\n",
    "    \"ZMALE\": {\"name\": \"zero-inflated MALE\", \"f\": ziloglaplace_ll},\n",
    "    \"MARE\": {\"name\": \"mean absolute square root error\", \"f\": mare_ll},\n",
    "}\n",
    "\n",
    "obj_df = pd.DataFrame.from_dict(objectives, orient='index')\n",
    "\n",
    "# step 2: compute the information in each objective function\n",
    "for index, row in obj_df.iterrows():\n",
    "    # nats is the negative log likelihood or the info in the error\n",
    "    #obj_df.loc[index, 'bits'] = - row.f(df.obs, df.NNDAR)/len(df)/np.log(2)\n",
    "    ll = row.f(df.obs, df.NNDAR)\n",
    "    obj_df.loc[index, 'bits'] = compute_info(ll, len(df))\n",
    "    \n",
    "# step 3: compute weights\n",
    "obj_df['weight'] = compute_weights(-obj_df.bits, base=2)\n",
    "\n",
    "# step 4: format output table\n",
    "\n",
    "table = obj_df[['name','bits','weight']].sort_values('weight').round(2)#.rename(columns=names)\n",
    "\n",
    "table['rank'] = len(table) - np.argsort(table['weight'])\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a54d5c5-8c3a-49e0-9883-3d366428b512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute noise in MSE\n",
    "nse_bits = table.loc[\"NSE\", \"bits\"]\n",
    "zmale_bits = table.loc[\"ZMALE\", \"bits\"]\n",
    "nse_noise = round(100 * (nse_bits - zmale_bits) / nse_bits)\n",
    "print(\n",
    "    f\"NSE is at least {nse_noise} percent noise\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
