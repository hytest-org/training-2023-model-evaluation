{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6482af7-ea2f-46a8-9ba3-2d9c7cfd2f91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Select an objective\n",
    "\n",
    "## Overview\n",
    "The first step in model evaluation is to select an appropriate model for representing the error (or uncertainty) in your model;\n",
    "in other words, select an objective function.\n",
    "\n",
    "For example, if the errors are normally distributed and /iid/, \n",
    "then the optimal objective function is mean squared error. \n",
    "We'll review why shortly. \n",
    "However, for many models, the error distribution is neither normal nor iid.\n",
    "Runoff models being a pertenant example.\n",
    "Their errors tend to scale with the magnitude of flow (heteroscedasticity).\n",
    "Logging the data helps to mitigate that,\n",
    "which is what most hydrologists want,\n",
    "even if they don't do it in practice.\n",
    "\n",
    "\n",
    "The next sections will demonstrate a basic procedure for selecting a reasonable objective function for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148f059-51b5-4316-b950-a0ad57e2ec78",
   "metadata": {},
   "source": [
    "## Benchmarking objectives\n",
    "Pretend that a model has a MSE of 1 and a MSLE of 1.\n",
    "Which is a better score?\n",
    "The short answer is we don't know.\n",
    "These scores have different scales, so we can't compare them directly.\n",
    "\n",
    "\n",
    "Fortunately, we already have everything we need.\n",
    "The trick is to transform the objectives into likelihoods,\n",
    "putting them all on a common scale.\n",
    "We'll demonstrate the simplest approach of using maximum likelihood estimators.\n",
    "\n",
    "### log likelihoods\n",
    "The first, and arguably de facto, objective is MSE, \n",
    "which corresponds to the log likelihood of the normal distribution,\n",
    "\\begin{equation}\n",
    "\\ell_2 = -n \\ln \\sigma  - \\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\text{,}\n",
    "\\end{equation}\n",
    "where $y_i$ are the observations, $\\hat y_i$ are the model predictions, and $\\sigma$ is standard deviation of the error.\n",
    "The final term is the most important.\n",
    "Stare at it for a moment and you'll recognize it as the L2 norm,\n",
    "which is essentially just the MSE.\n",
    "The remaining terms normalize the result, which we need in order to compare other objective functions.\n",
    "If we calibrate a model to just MSE, we could drop these terms.\n",
    "However, to compare different objectives, we need to keep them.\n",
    "\n",
    "Another common objective is mean absolute error (MAE),\n",
    "which corresponds to the log likelihood of the Laplace distribution,\n",
    "\\begin{equation}\n",
    "\\ell_1 = -n \\ln(2b)  - \\frac{1}{b} \\sum_{i=1}^n | y_i - \\hat y_i| \\text{,}\n",
    "\\end{equation}\n",
    "where $b$ is mean absolute error\n",
    "(also known as the L1 norm).\n",
    "\n",
    "### Changing variables\n",
    "Likelihoods for a variety of other objective functions are obtained by changing variables.\n",
    "For example, the mean squared log error (MSLE), which corresponds to the lognormal log likelihood $\\ell_3$,\n",
    "is obtained from $\\ell_2$ by changing variables\n",
    "\\begin{equation}\n",
    "\\ell_3 = \\ell_2(v(y)) + \\ln|v'(y)| \\text{,}\n",
    "\\end{equation}\n",
    "where $v$, the natural log in this case.\n",
    "\n",
    "Can you define more?\n",
    "\n",
    "### Additional reading\n",
    "```\n",
    "@Article{Hodson_2022,\n",
    "  doi = {10.48550/ARXIV.2212.06566},\n",
    "  author = {Hodson, Timothy O. and Over, Thomas M. and Smith, Tyler J. and Marshall, Lucy M.},\n",
    "  title = {How to select an objective function using information theory},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9e49e-dc18-4662-82d4-be76701047a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data\n",
    "First load a dataset to experiment with. Here we'll compare NNDAR against streamgage observations (obs). By nature, the predictions from NNDAR are 'out of sample'; otherwise, we'd need to use cross validation or information criteria to estimate the out-of-sample performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512a667a-1d4a-42ee-a3fb-1b2fd2d0e226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>NNDAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gage_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">01013500</th>\n",
       "      <th>1980-10-01</th>\n",
       "      <td>509.0</td>\n",
       "      <td>649.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-02</th>\n",
       "      <td>518.0</td>\n",
       "      <td>618.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-03</th>\n",
       "      <td>516.0</td>\n",
       "      <td>618.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-04</th>\n",
       "      <td>620.0</td>\n",
       "      <td>796.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-05</th>\n",
       "      <td>759.0</td>\n",
       "      <td>1397.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">402114105350101</th>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>23.4</td>\n",
       "      <td>23.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>22.1</td>\n",
       "      <td>22.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>23.8</td>\n",
       "      <td>26.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>28.4</td>\n",
       "      <td>26.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>25.5</td>\n",
       "      <td>25.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14067063 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              obs    NNDAR\n",
       "gage_id         date                      \n",
       "01013500        1980-10-01  509.0   649.55\n",
       "                1980-10-02  518.0   618.91\n",
       "                1980-10-03  516.0   618.91\n",
       "                1980-10-04  620.0   796.61\n",
       "                1980-10-05  759.0  1397.14\n",
       "...                           ...      ...\n",
       "402114105350101 2017-09-26   23.4    23.32\n",
       "                2017-09-27   22.1    22.79\n",
       "                2017-09-28   23.8    26.72\n",
       "                2017-09-29   28.4    26.99\n",
       "                2017-09-30   25.5    25.15\n",
       "\n",
       "[14067063 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from s3 (run once)\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "fs_read = fsspec.filesystem(\n",
    "    \"s3\", anon=True, client_kwargs={\"endpoint_url\": \"https://renc.osn.xsede.org\"}\n",
    ")\n",
    "\n",
    "with fs_read.open(\"s3://rsignellbucket2/hytest/thodson/gages2_nndar.parquet\") as f:\n",
    "    df = pd.read_parquet(f)\n",
    "\n",
    "# threshold low values\n",
    "df[df < 0.01] = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8ab259b-2a54-452a-b973-1dbcc8efa4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>obs</th>\n",
       "      <th>NNDAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gage_id</th>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">01013500</th>\n",
       "      <th>1980-10-01</th>\n",
       "      <td>509.0</td>\n",
       "      <td>649.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-02</th>\n",
       "      <td>518.0</td>\n",
       "      <td>618.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-03</th>\n",
       "      <td>516.0</td>\n",
       "      <td>618.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-04</th>\n",
       "      <td>620.0</td>\n",
       "      <td>796.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-10-05</th>\n",
       "      <td>759.0</td>\n",
       "      <td>1397.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-26</th>\n",
       "      <td>227.0</td>\n",
       "      <td>90.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27</th>\n",
       "      <td>224.0</td>\n",
       "      <td>72.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28</th>\n",
       "      <td>209.0</td>\n",
       "      <td>128.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29</th>\n",
       "      <td>188.0</td>\n",
       "      <td>110.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30</th>\n",
       "      <td>178.0</td>\n",
       "      <td>105.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13514 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       obs    NNDAR\n",
       "gage_id  date                      \n",
       "01013500 1980-10-01  509.0   649.55\n",
       "         1980-10-02  518.0   618.91\n",
       "         1980-10-03  516.0   618.91\n",
       "         1980-10-04  620.0   796.61\n",
       "         1980-10-05  759.0  1397.14\n",
       "...                    ...      ...\n",
       "         2017-09-26  227.0    90.11\n",
       "         2017-09-27  224.0    72.09\n",
       "         2017-09-28  209.0   128.40\n",
       "         2017-09-29  188.0   110.38\n",
       "         2017-09-30  178.0   105.88\n",
       "\n",
       "[13514 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select one gage for testing\n",
    "gage = df[df.index.get_level_values(0) == \"01013500\"]\n",
    "gage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93e76d-12bb-4130-9a35-ec1a1d3568fb",
   "metadata": {},
   "source": [
    "Now compute the log likelihood assuming the errors follow a normal distribution, which is equivalent to using MSE as an objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3ef5f69-426e-4ebe-a9f7-bcec144a0531",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normal_ll(y, y_hat):\n",
    "    \"\"\"Compute log likelihood for the normal distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        Log likelihood\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "normal_ll(gage[\"obs\"], gage[\"NNDAR\"])  # should return -121363.3792673729"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e4d46-9b44-4c0f-b2ea-c2dbec52420f",
   "metadata": {},
   "source": [
    "For extra credit, define `normal_ll` to accept a change of variable. Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cb58f9-abce-4095-b7cf-e89b8f2d2591",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def normal_ll(y, y_hat, transform=None, gradient=1):\n",
    "    \"\"\"Log likelihood for the normal distribution with change of variable\n",
    "\n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "\n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = (\n",
    "        -n * np.log(sigma)\n",
    "        - n / 2 * np.log(2 * np.pi)\n",
    "        - 1 / (2 * sigma**2) * (e**2).sum()\n",
    "        + log_gradient\n",
    "    )\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54f93c-f583-461e-96a7-9ebe8248d2c5",
   "metadata": {},
   "source": [
    "## Other likelihoods\n",
    "Now implement some other log likelihoods.\n",
    "\n",
    "Hint: Ideally use a change of variable, but you can also hard code each distribution. For the lognormal, see the PDF at https://en.wikipedia.org/wiki/Log-normal_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a6f59f-b19d-4273-9749-001cbbced3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_ll(y, y_hat):\n",
    "    pass\n",
    "\n",
    "\n",
    "def lognormal_ll(y, y_hat):\n",
    "    pass\n",
    "\n",
    "\n",
    "def loglaplace_ll(y, y_hat):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b326c-8234-41c0-8dbf-d54b095b6944",
   "metadata": {},
   "source": [
    "Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffa7e15f-01c7-4810-adab-7d8e2fc1af95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute likelihood\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def normal_ll(y, y_hat, transform=None, gradient=1):\n",
    "    \"\"\"Log likelihood for the normal distribution with change of variable\n",
    "\n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "\n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "\n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = (\n",
    "        -n * np.log(sigma)\n",
    "        - n / 2 * np.log(2 * np.pi)\n",
    "        - 1 / (2 * sigma**2) * (e**2).sum()\n",
    "        + log_gradient\n",
    "    )\n",
    "    return ll\n",
    "\n",
    "\n",
    "def laplace_ll(y, y_hat, transform=None, gradient=1):\n",
    "    \"\"\"Log likelihood for Laplace distribution with change of variable\n",
    "\n",
    "    The laplace distribution is the formal likelihood for the mean absolute\n",
    "    error (MAE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "    \"\"\"\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "\n",
    "    e = (y - y_hat).abs()\n",
    "    n = len(e)\n",
    "    b = e.mean()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = -n * np.log(2 * b) - 1 / b * e.sum() + log_gradient\n",
    "    return ll.sum()\n",
    "\n",
    "\n",
    "def msre_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for mean squared square-root error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return normal_ll(\n",
    "        y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1 / (2 * np.sqrt(y))\n",
    "    )\n",
    "\n",
    "\n",
    "def mare_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for mean absolute square-root error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return laplace_ll(\n",
    "        y, y_hat, transform=lambda x: np.sqrt(x), gradient=-1 / (2 * np.sqrt(y))\n",
    "    )\n",
    "\n",
    "\n",
    "def lognormal_ll(y, y_hat):\n",
    "    \"\"\"Lognormal log likelihood\n",
    "\n",
    "    The lognormal distribution is the formal likelihood for the mean squared\n",
    "    log error (MSLE).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return normal_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1 / y)\n",
    "\n",
    "\n",
    "def mspe_ll(y, y_hat):\n",
    "    \"\"\"Log likelhood for mean squared percentage error\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "\n",
    "    \"\"\"\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x / y, gradient=-1 / (y**2))\n",
    "\n",
    "\n",
    "def nse_ll(y, y_hat, group=\"gage_id\"):\n",
    "    \"\"\"Log likelihood for normalized squared error (NSE)\n",
    "\n",
    "    NSE is equivalent to the Nash–Sutcliffe model efficiency coefficient.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    sigma_o = y.groupby(\"gage_id\").transform(lambda x: x.std())\n",
    "    return normal_ll(y, y_hat, transform=lambda x: x / sigma_o, gradient=1 / sigma_o)\n",
    "\n",
    "\n",
    "def loglaplace_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for log Laplace distribution\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return laplace_ll(y, y_hat, transform=lambda x: np.log(x), gradient=1 / y)\n",
    "\n",
    "\n",
    "def uniform_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for uniform distribution.\n",
    "\n",
    "    The uniform log likelihood minimizes the maximum error.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    e = np.abs(y - y_hat)\n",
    "    n = len(e)\n",
    "    # ll = -n * np.log(e.max()-e.min()) # standard formulation\n",
    "    ll = -n * np.log(e.max() - 0)\n",
    "    return ll\n",
    "\n",
    "\n",
    "def bernoulli_ll(y, y_hat, groupby=None):\n",
    "    \"\"\"TODO and use within zi_ll\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def zi_ll(y, y_hat, ll=normal_ll, threshold=0.01, groupby=None):\n",
    "    \"\"\"Zero-inflated log likelihood.\n",
    "\n",
    "     Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    ll : function\n",
    "        Zero-inflated log likelihood\n",
    "    threshold : float\n",
    "        Value below which is treated as zero\n",
    "    groupby : string\n",
    "        Optional groupby term (testing)\n",
    "    \"\"\"\n",
    "    y_o = y <= threshold\n",
    "    y_hat_o = y_hat <= threshold\n",
    "\n",
    "    if groupby is None:\n",
    "        n1 = (y_o & y_hat_o).sum()  # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).sum()  # incorrect zero-flow prediction\n",
    "    else:\n",
    "        n1 = (y_o & y_hat_o).groupby(groupby).sum()  # correct zero-flow prediction\n",
    "        n2 = (y_o ^ y_hat_o).groupby(groupby).sum()  # incorrect zero-flow prediction\n",
    "\n",
    "    n3 = ~y_o & ~y_hat_o  # correct flow predictions\n",
    "\n",
    "    # fraction of correctly predicted zero flows\n",
    "    rho = np.where((n1 + n2) == 0, 0, n1 / (n1 + n2))\n",
    "    n_rho = 1 - rho\n",
    "\n",
    "    # n1 * np.log(rho) + n2 * np.log(1-rho)\n",
    "    ll_zero = n1[rho != 0] * np.log(rho[rho != 0]) + n2[n_rho != 0] * np.log(\n",
    "        n_rho[n_rho != 0]\n",
    "    )\n",
    "\n",
    "    return ll_zero.sum() + ll(y[n3], y_hat[n3])\n",
    "\n",
    "\n",
    "def zilognormal_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for zero-inflated lognormal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "\n",
    "    return zi_ll(y, y_hat, ll=lognormal_ll, threshold=0.01)\n",
    "\n",
    "\n",
    "def ziloglaplace_ll(y, y_hat):\n",
    "    \"\"\"Log likelihood for zero-inflated laplace.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "    y_hat : array_like\n",
    "    \"\"\"\n",
    "    return zi_ll(y, y_hat, ll=loglaplace_ll, threshold=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d767c5-98f3-4322-8362-3734a7867bbd",
   "metadata": {},
   "source": [
    "To evaluate different objectives, we compare their log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b593b3f0-c33c-4458-9bfb-423da35e99d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>mean squared error</td>\n",
       "      <td>-1.132713e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>mean absolute error</td>\n",
       "      <td>-9.256028e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>mean squared log error</td>\n",
       "      <td>-7.280278e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALE</th>\n",
       "      <td>mean absolute log error</td>\n",
       "      <td>-6.864250e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         name            ll\n",
       "MSE        mean squared error -1.132713e+08\n",
       "MAE       mean absolute error -9.256028e+07\n",
       "MSLE   mean squared log error -7.280278e+07\n",
       "MALE  mean absolute log error -6.864250e+07"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: create a table of objective functions\n",
    "objectives = {\n",
    "    \"MSE\": {\"name\": \"mean squared error\", \"f\": normal_ll},\n",
    "    \"MAE\": {\"name\": \"mean absolute error\", \"f\": laplace_ll},\n",
    "    \"MSLE\": {\"name\": \"mean squared log error\", \"f\": lognormal_ll},\n",
    "    \"MALE\": {\"name\": \"mean absolute log error\", \"f\": loglaplace_ll},\n",
    "}\n",
    "\n",
    "obj_df = pd.DataFrame.from_dict(objectives, orient=\"index\")\n",
    "\n",
    "# step 2: compute the information in each objective function\n",
    "for index, row in obj_df.iterrows():\n",
    "    # nats is the negative log likelihood or the info in the error\n",
    "    obj_df.loc[index, \"ll\"] = row.f(df.obs, df.NNDAR)\n",
    "\n",
    "obj_df = obj_df[[\"name\", \"ll\"]]\n",
    "\n",
    "obj_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bc084-bdd1-41ee-855d-6a6d3159d88b",
   "metadata": {},
   "source": [
    "Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876eec1e-da19-461a-8480-4dd4d903955d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# paste into step 1\n",
    "objectives = {\n",
    "    \"U\": {\"name\": \"uniformly distributed error\", \"f\": uniform_ll},\n",
    "    \"MSE\": {\"name\": \"mean squared error\", \"f\": normal_ll},\n",
    "    \"NSE\": {\"name\": \"normalized squared error\", \"f\": nse_ll},\n",
    "    \"MAE\": {\"name\": \"mean absolute error\", \"f\": laplace_ll},\n",
    "    \"MSPE\": {\"name\": \"mean squared percent error\", \"f\": mspe_ll},\n",
    "    \"MSLE\": {\"name\": \"mean squared log error*\", \"f\": lognormal_ll},\n",
    "    \"MALE\": {\"name\": \"mean absolute log error*\", \"f\": loglaplace_ll},\n",
    "    \"ZMSLE\": {\"name\": \"zero-inflated MSLE\", \"f\": zilognormal_ll},\n",
    "    \"ZMALE\": {\"name\": \"zero-inflated MALE\", \"f\": ziloglaplace_ll},\n",
    "    \"MARE\": {\"name\": \"mean absolute square root error\", \"f\": mare_ll},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b14e7-447c-4b42-b8e7-d9107bc623b2",
   "metadata": {},
   "source": [
    "## Akaike weights\n",
    "\n",
    "In practice, we aren't intersted in the magnitude of the log likelihood, only their differences.\n",
    "One approach is to represent each by its Akaike weights, which represents the probability that each objective is the correct on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b16ac8-609e-43c8-a116-3663646e64ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weights(series, base=np.e):\n",
    "    \"\"\"Compute posterior weights\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array_like\n",
    "        Log likelihoods\n",
    "    base: float\n",
    "        Base of the logarithm used to compute log likelihood\n",
    "    \"\"\"\n",
    "    s = base**series\n",
    "    return s / s.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb86bfe2-b0dc-40a6-a6d9-463d0d9baeea",
   "metadata": {},
   "source": [
    "## Bits\n",
    "\n",
    "We can also convert the log likelihood to bits. Its magnitude is still meaningless, but at least its scale is more digestable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9b7cea5-72ac-4dd6-9a6d-b5ca8d97898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_info(ll, n, base=2):\n",
    "    \"\"\"Convert a log likelihood to bits\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ll : array_like\n",
    "        Log likelihoods\n",
    "    n :\n",
    "    base: float\n",
    "        Base of the logarithm; 2 = bits\n",
    "    \"\"\"\n",
    "    return -ll / n / np.log(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9176057b-979a-4cf5-801e-b8348ff98533",
   "metadata": {},
   "source": [
    "Now we're ready to generate some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f987eeec-2a42-431f-9ed9-b2844be2b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>bits</th>\n",
       "      <th>weight</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>mean squared error</td>\n",
       "      <td>11.62</td>\n",
       "      <td>0.02</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>mean absolute error</td>\n",
       "      <td>9.49</td>\n",
       "      <td>0.09</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSLE</th>\n",
       "      <td>mean squared log error*</td>\n",
       "      <td>7.47</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MALE</th>\n",
       "      <td>mean absolute log error*</td>\n",
       "      <td>7.04</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name   bits  weight  rank\n",
       "MSE         mean squared error  11.62    0.02     4\n",
       "MAE        mean absolute error   9.49    0.09     3\n",
       "MSLE   mean squared log error*   7.47    0.38     2\n",
       "MALE  mean absolute log error*   7.04    0.51     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: create a table of objective functions\n",
    "objectives = {\n",
    "    'MSE' : {'name':'mean squared error', 'f':normal_ll},\n",
    "    'MAE' : {'name': 'mean absolute error', 'f':laplace_ll},\n",
    "    'MSLE' : {'name':'mean squared log error*', 'f':lognormal_ll},\n",
    "    'MALE' : {'name':'mean absolute log error*', 'f':loglaplace_ll}\n",
    "}\n",
    "\n",
    "obj_df = pd.DataFrame.from_dict(objectives, orient='index')\n",
    "\n",
    "# step 2: compute the information in each objective function\n",
    "for index, row in obj_df.iterrows():\n",
    "    # nats is the negative log likelihood or the info in the error\n",
    "    #obj_df.loc[index, 'bits'] = - row.f(df.obs, df.NNDAR)/len(df)/np.log(2)\n",
    "    ll = row.f(df.obs, df.NNDAR)\n",
    "    obj_df.loc[index, 'bits'] = compute_info(ll, len(df))\n",
    "    \n",
    "# step 3: compute weights\n",
    "obj_df['weight'] = compute_weights(-obj_df.bits, base=2)\n",
    "\n",
    "# step 4: format output table\n",
    "\n",
    "table = obj_df[['name','bits','weight']].sort_values('weight').round(2)#.rename(columns=names)\n",
    "\n",
    "table['rank'] = len(table) - np.argsort(table['weight'])\n",
    "\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeee4886-e3c6-46a3-af78-eee74b324649",
   "metadata": {},
   "source": [
    "Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f55b5678-e377-4bef-88a8-d7df4f4f79c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "objectives = {\n",
    "\n",
    "    'MSE' : {'name':'mean squared error', 'f':normal_ll},\n",
    "    'MAE' : {'name': 'mean absolute error', 'f':laplace_ll},\n",
    "    'MSPE' : {'name': 'mean squared percent error', 'f':mspe_ll},\n",
    "    'MSLE' : {'name':'mean squared log error*', 'f':lognormal_ll},\n",
    "    'MALE' : {'name':'mean absolute log error*', 'f':loglaplace_ll},\n",
    "    'ZMSLE' : {'name':'zero-inflated MSLE', 'f':zilognormal_ll},\n",
    "    'ZMALE' : {'name':'zero-inflated MALE', 'f':ziloglaplace_ll},\n",
    "    'MARE' : {'name':'mean absolute square root error', 'f':mare_ll},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a54d5c5-8c3a-49e0-9883-3d366428b512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is at least 39 percent noise\n"
     ]
    }
   ],
   "source": [
    "# compute noise in MSE\n",
    "mse_bits = table.loc[\"MSE\", \"bits\"]\n",
    "male_bits = table.loc[\"MALE\", \"bits\"]\n",
    "mse_noise = round(100 * (mse_bits - male_bits) / mse_bits)\n",
    "print(\n",
    "    f\"MSE is at least {mse_noise} percent noise\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
