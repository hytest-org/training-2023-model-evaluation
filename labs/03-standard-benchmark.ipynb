{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Workflow :: NWM streamflow Analysis\n",
    "\n",
    "<img src='../images/Eval_Analysis.svg' width=600>\n",
    "\n",
    "## Essential Benchmark Components\n",
    "This benchmark notebook will present a workflow which follows a canonical model for Essential Benchmark Components: \n",
    "1) A set of predictions and matching observation (the data); \n",
    "2) The domain (e.g. space or time) over which to benchmark;\n",
    "3) A set of statistical metrics with which to benchmark. \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load libraries and configure Python computing environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Data\n",
    "<img src='../images/Eval_Analysis_Data.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1. **a set of predictions and matching observations;**\n",
    "1. a domain over which to benchmark;\n",
    "1. a set of statistical metrics with which to benchmark.\n",
    "\n",
    "Finding and loading data is made easier for this particular workflow (the _streamflow_ variable), in that most of it has been \n",
    "pre-processed and stored in a cloud-friendly format.  That process is described in the [data preparation](./00-data-prep.ipynb)\n",
    "notebook. We will proceed here using the already-prepared data for _streamflow_, which is included in the HyTEST [`intake`](https://intake.readthedocs.io/en/latest/index.html) catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import intake\n",
    "cat = intake.open_catalog(r'https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml')\n",
    "print(\"Available datasets: \\n\", \"\\n\".join(cat.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above list represents the processed datasets available for benchmarking.  If a dataset\n",
    "you want is not in that list, \n",
    "you can load the data manually via direct connection to 'on-prem' or S3 object storage. \n",
    "If you load data from a source other than this list, you can jump to [Step 2: Restrict to a Domain](#step-2-restrict-to-a-domain)\n",
    "\n",
    "Note that the interesting datasets in the cataloged dataset above are duplicated: Some are `-onprem` \n",
    "and some are `-cloud`. Same data, but the storage location and access protocol will be different. You \n",
    "will definitely want to use the correct copy of the data for your computing environment.  \n",
    "* `onprem` : Direct access via the `caldera` filesystem from _denali_ or _tallgrass_\n",
    "* `cloud` : Network access via S3 bucket, suitable for consumption on cloud-hosted jupyter servers. You could also access using any network-attached computer, but the amount of data will likely saturate your connection.  Use in the cloud (e.g. ESIP QHub)\n",
    "\n",
    "So... are you on-prem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "onprem = (platform.node() in ['denali', 'tallgrass'])  \n",
    "# NOTE: these hostnames are not quite correct... this will always return not onprem.\n",
    "if onprem:\n",
    "    print(\"Yes : -onprem\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-onprem'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-onprem'\n",
    "else:\n",
    "    print(\"Not onprem; use '-cloud' data source\")\n",
    "    obs_data_src='nwis-streamflow-usgs-gages-cloud'\n",
    "    mod_data_src='nwm21-streamflow-usgs-gages-cloud'\n",
    "print(\"Observed : \", obs_data_src)\n",
    "print(\"Modeled  : \", mod_data_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variable_of_interest = 'streamflow'\n",
    "try:\n",
    "    obs = cat[obs_data_src].to_dask()\n",
    "    mod = cat[mod_data_src].to_dask()\n",
    "except KeyError:\n",
    "    print(\"Something wrong with dataset names.\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    obs_data = obs[variable_of_interest]\n",
    "    mod_data = mod[variable_of_interest].astype('float32')\n",
    "except KeyError:\n",
    "    print(f\"{variable_of_interest} was not found in these data.\")\n",
    "\n",
    "obs_data.name = 'observed'\n",
    "mod_data.name = 'predicted'    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Restrict to a Domain\n",
    "\n",
    "<img src='../images/Eval_Analysis_Domain.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1. a set of predictions and matching observations;\n",
    "1. **a domain over which to benchmark;**\n",
    "1. a set of statistical metrics with which to benchmark.\n",
    "\n",
    "Each benchmark domain is defined over specific bounds (typically space and/or time). \n",
    "Benchmark domain definitions can be defined within the notebook, or sourced from\n",
    "elsewhere. For this example, we use the _Cobalt_ gages, avaliable for download on ScienceBase \n",
    "([Foks et al., 2022](https://doi.org/10.5066/P972P42Z)), as our 'spatial domain'.  \n",
    "\n",
    "This is essentially a list of \n",
    "stream gages in which we are interested, along with some  metadata about that gage (watershed, \n",
    "reach code, etc).  We will use this as a spatial selector to restrict the original data to only \n",
    "those gages found within this benchmarking domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cobalt = pd.read_csv(\n",
    "    'https://www.sciencebase.gov/catalog/file/get/6181ac65d34e9f2789e44897?f=__disk__22%2F1a%2Fd2%2F221ad2fe9d95de17731ad35d0fc6b417a4b53ee1',\n",
    "    dtype={'site_no':str, 'huc_cd':str, 'reachcode':str, 'comid':str, 'gagesII_class':str, 'aggecoregion': str}, \n",
    "    index_col='site_no'\n",
    "    )\n",
    "# Re-format the gage_id/site_no string value.  ex:   \"1000000\"  ==> \"USGS-1000000\"\n",
    "cobalt.rename(index=lambda x: f'USGS-{x}', inplace=True)\n",
    "print(f\"{len(cobalt.index)} gages in this benchmark\")\n",
    "cobalt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a domain dataset representing the stream gages (unique `site_no` values) identifying the locations making up the spatial domain of this benchmark. While we have good meta-data for these gages (lat/lon location, HUC8 code, etc), we really will only use the list of `site_no` values to select locations out of the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our temporal domain or extent for the evaluation. Let's specify a certain time period below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter in start and end dates\n",
    "start_time = '1983-10-01'\n",
    "end_time='2016-12-31'\n",
    "\n",
    "# from our available data, select a start and end time to restrict the temporal domain/extent of the assessment.\n",
    "obs_data = obs_data.sel(time=slice(start_time,end_time))\n",
    "mod_data = mod_data.sel(time=slice(start_time,end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Metrics\n",
    "\n",
    "<img src='../images/Eval_Analysis_Metrics.svg' width=360>\n",
    "\n",
    "Essential Benchmark Components: \n",
    "1. a set of predictions and matching observations;\n",
    "1. a domain over which to benchmark;\n",
    "1. **a set of statistical metrics with which to benchmark.**\n",
    "\n",
    "The specific metrics for the NWM \"Standard Suite\" have been codified for use in Python Jupyter notebooks\n",
    "[here](../supplement/standard-benchmark-definitions.ipynb).  You can see examples of how these might be used in \n",
    "[this notebook](../supplement/standard-benchmark-usage.ipynb). \n",
    "\n",
    "You can use these pre-coded metrics or write your own. To use the Standard Suite, we can run their\n",
    "definition notebook in the following cell. This will make its contents available to us in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run ../supplement/standard-benchmark-definitions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Whether you use these functions or your own, we need to put all metric computation into a single all-encompasing \n",
    "benchmarking function--a single call which can be assigned to each gage in our domain list. This sort of arrangement \n",
    "is well-suited to parallelism with `dask`. \n",
    "\n",
    "If this is done well, the process will benefit enormously from task parallelism -- each gage can be given its own \n",
    "CPU on which to run. After all are done, the various results will be collected and assembled into a composite dataset. \n",
    "\n",
    "To achieve this, we need a single 'atomic' function that can execute independently. It will take the gage identifier \n",
    "as input and return a list of metrics from the Standard Suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Wrapper function -- this func will be called once per gage_id, each call on its own dask worker\n",
    "def compute_benchmark(gage_id):\n",
    "    try:\n",
    "        ## obs_data and mod_data should be globals...\n",
    "        obs = obs_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series()\n",
    "        mod = mod_data.sel(gage_id=gage_id).load(scheduler='single-threaded').to_series().resample('1D', offset='5h').mean() \n",
    "        \n",
    "        # make sure the indices match\n",
    "        # this is a rough idea of \n",
    "        obs.index = obs.index.to_period('D')\n",
    "        mod.index = mod.index.to_period('D')\n",
    "\n",
    "        # merge obs and predictions; drop NaNs.\n",
    "        gage_df = pd.merge(obs, mod, left_index=True, right_index=True).dropna(how='any')\n",
    "        \n",
    "        scores = pd.Series(\n",
    "            data={\n",
    "                'NSE': NSE(gage_df.observed, gage_df.predicted),\n",
    "                'KGE': KGE(gage_df.observed, gage_df.predicted),\n",
    "                'logNSE': logNSE(gage_df.observed, gage_df.predicted),\n",
    "                'pbias': pbias(gage_df.observed, gage_df.predicted),\n",
    "                'rSD': rSD(gage_df.observed, gage_df.predicted),\n",
    "                'pearson': pearson_r(gage_df.observed, gage_df.predicted),\n",
    "                'spearman': spearman_r(gage_df.observed, gage_df.predicted), \n",
    "                'pBiasFMS': pBiasFMS(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFLV': pBiasFLV(gage_df.observed, gage_df.predicted),\n",
    "                'pBiasFHV': pBiasFHV(gage_df.observed, gage_df.predicted)\n",
    "            },\n",
    "            name=gage_id,\n",
    "            dtype='float64'\n",
    "        )\n",
    "        return scores\n",
    "    \n",
    "    except Exception as e:#<-- this is an extremely broad way to catch exceptions.  We only do it this way to ensure \n",
    "                          #    that a failure on one benchmark (for a single stream gage) will not halt the entire run. \n",
    "        logging.info(\"Benchmark failed for %s\", gage_id)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test to be sure this `compute_benchmark()` function will return data for a single gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_benchmark('USGS-01030350')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the Analysis \n",
    "\n",
    "We now need to set up a way to farm out this function, called once per gage ID, to workers in a \n",
    "parallel cluster. `dask` will do this using a dask '_bag_'.  \n",
    "\n",
    "Read more about task parallelism with Dask and how we are using dask bags [here](https://docs.dask.org/en/stable/bag.html)\n",
    "\n",
    "\n",
    "The cluster configuration is dependent on your AWS profile and credentials.  First, establish AWS identity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ### ### CONFIG CUSTOM CLUSTER HERE ### ### ###\n",
    "aws_profile = 'usgs-scratch'\n",
    "aws_region = 'us-west-2'\n",
    "aws_endpoint = 'https://renc.osn.xsede.org'\n",
    "  \n",
    "import configparser\n",
    "awsconfig = configparser.ConfigParser()\n",
    "awsconfig.read(\n",
    "    os.path.expanduser(r'~/.aws/credentials') # default location... if yours is elsewhere, change this.\n",
    ")\n",
    "# Set environment vars based on parsed awsconfig\n",
    "os.environ['AWS_ACCESS_KEY_ID']     = awsconfig[aws_profile]['aws_access_key_id']    \n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = awsconfig[aws_profile]['aws_secret_access_key']    \n",
    "os.environ['AWS_S3_ENDPOINT']       = aws_endpoint\n",
    "os.environ['AWS_DEFAULT_REGION']    = aws_region\n",
    "\n",
    "try: \n",
    "    del os.environ['AWS_PROFILE']\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With identity established, we can spin up a cluster of workers, accessed via a Dask `gateway`\n",
    "\n",
    "This next cell may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "gateway = Gateway()\n",
    "options = gateway.cluster_options()\n",
    "options.conda_environment='users/users-pangeo'\n",
    "options.profile = 'Medium Worker'\n",
    "options.environment_vars = dict(\n",
    "    DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION=\"1.0\"\n",
    ")\n",
    "options.environment_vars.update(dict(os.environ))\n",
    "cluster = gateway.new_cluster(options)\n",
    "cluster.adapt(minimum=1, maximum=15)\n",
    "\n",
    "# get the client for the cluster\n",
    "client = cluster.get_client()\n",
    "client.dashboard_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If needed, you can adapt the max number of workers to speed up the problem.\n",
    "# cluster.adapt(minimum=1, maximum=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Set up a dask bag with the contents being a list of the cobalt gages\n",
    "import dask.bag as db\n",
    "\n",
    "## Create a dask Bag from Python sequence.\n",
    "bag = db.from_sequence(cobalt.index.tolist(), partition_size=30).map(compute_benchmark)\n",
    "\n",
    "results = bag.compute() #<< Dispatch the workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that big task done, we don't need `dask` parallelism any more. Let's shut down the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close(); del client\n",
    "cluster.close(); del cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the results\n",
    "The `bag` now contains a collection of return values (one per call to `compute_benchmark()`).\n",
    "We can massage that into a table/dataframe for easier processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = [i for i in results if i is not None] # Drop entries where compute_benchmark failed\n",
    "results_df = pd.concat(r, axis=1).T\n",
    "results_df.index.name = 'site_no'\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe/table can be saved to disk as a CSV. It will be used for visualizations in [other notebooks](NWM_Benchmark_Viz.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df.to_csv('NWM_v2.1_streamflow_example.csv') ##<--- change this to a personalized filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This csv file will be used in the 06-benchmark-visualization notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "517e0b9691bfb9ddc6621127e315efa61ab1d06d511ccc3555feb3821bb7e046"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
