{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17a80bd0-ab2d-49e2-8d23-0e3e7bf3d58c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# d-score\n",
    "\n",
    "## Overview\n",
    "This lab \n",
    "\n",
    "Steps\n",
    "1. Decompose your error into orthogonal components\n",
    "2. Pass each component through a scoring function\n",
    "\n",
    "In fact, the order doesn't matter, which is one of the keys to `d-score`'s flexibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6482af7-ea2f-46a8-9ba3-2d9c7cfd2f91",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Select your objective\n",
    "The first step in the `d-score` approach is to select an appropriate distribution to represent the error (or uncertainty) in your model predictions.\n",
    "If this all sounds unfamiliar, relax;\n",
    "we're really just asking \"what is the best objective function for your model?\"\n",
    "\n",
    "For example, if the errors are normally distributed and /iid/, \n",
    "then the optimal objective function (or benchmark) is mean squared error. \n",
    "We'll review why shortly. \n",
    "However, for many models, the error distribution is neither normal nor iid.\n",
    "Runoff models are a pertenant example.\n",
    "Their errors tend to scale with the magnitude of flow (violating /iid/).\n",
    "Logging the data can help remove that scaling effect,\n",
    "which is what most hydrologists want,\n",
    "even if their choice of objective says otherwise.\n",
    "\n",
    "The next sections will demonstrate a basic procedure for selecting a reasonable objective function for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7148f059-51b5-4316-b950-a0ad57e2ec78",
   "metadata": {},
   "source": [
    "## Benchmarking objectives\n",
    "A model has a MSE of 1 and a MSLE of 1.\n",
    "Which is a better score?\n",
    "The short answer is we don't know.\n",
    "These scores have different scales, so we can't compare them without more information.\n",
    "\n",
    "Fortunately, we already have all the information we need.\n",
    "The trick is to transform the objectives into likelihoods,\n",
    "thereby putting them all on a common scale.\n",
    "We'll demonstrate the simplest approach, which is to use maximum likelihood estimators.\n",
    "\n",
    "### log likelihoods\n",
    "The first, and arguably de facto, objective is MSE, \n",
    "which corresponds to the log likelihood of the normal distribution,\n",
    "\\begin{equation}\n",
    "\\ell_2 = -n \\ln \\sigma  - \\frac{n}{2} \\ln(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\hat y_i)^2 \\text{,}\n",
    "\\end{equation}\n",
    "where $y_i$ are the observations, $\\hat y_i$ are the model predictions, and $\\sigma$ is standard deviation of the error.\n",
    "The final term is the most important.\n",
    "Stare at it for a moment and you'll recognize it as the L2 norm,\n",
    "which is essentially just the MSE.\n",
    "The remaining terms normalize the result, which we need in order to compare other objective functions.\n",
    "If we calibrate a model to just MSE, we could drop these terms.\n",
    "However, to compare different objectives, we need to keep them.\n",
    "\n",
    "Another common objective is mean absolute error (MAE),\n",
    "which corresponds to the log likelihood of the Laplace distribution (Figure \\ref{figure1}),\n",
    "\\begin{equation}\n",
    "\\ell_1 = -n \\ln(2b)  - \\frac{1}{b} \\sum_{i=1}^n | y_i - \\hat y_i| \\text{,}\n",
    "\\end{equation}\n",
    "where $b$ is mean absolute error\n",
    "(also known as the L1 norm).\n",
    "\n",
    "### Changing variables\n",
    "Fortunately, most of the hard work\n",
    "Now, there are other \"classic\" probability distributions \n",
    "Likelihoods for a variety of other objective functions are obtained by changing variables.\n",
    "For example, the mean squared log error (MSLE), which corresponds to the lognormal log likelihood $\\ell_3$,\n",
    "is obtained from $\\ell_2$ by changing variables\n",
    "\\begin{equation}\n",
    "\\ell_3 = \\ell_2(v(y)) + \\ln|v'(y)| \\text{,}\n",
    "\\end{equation}\n",
    "where $v$, the natural log in this case.\n",
    "\n",
    "Can you define more?\n",
    "\n",
    "### Additional reading\n",
    "```\n",
    "@Article{Hodson_2022,\n",
    "  doi = {10.48550/ARXIV.2212.06566},\n",
    "  author = {Hodson, Timothy O. and Over, Thomas M. and Smith, Tyler J. and Marshall, Lucy M.},\n",
    "  title = {How to select an objective function using information theory},\n",
    "  publisher = {arXiv},\n",
    "  year = {2022}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa9e49e-dc18-4662-82d4-be76701047a9",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ef5f69-426e-4ebe-a9f7-bcec144a0531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_ll(y, y_hat):\n",
    "    \"\"\"Compute log likelihood for the normal distribution\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        Log likelihood\n",
    "        \n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    \"\"\"\n",
    "    return ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e4d46-9b44-4c0f-b2ea-c2dbec52420f",
   "metadata": {},
   "source": [
    "For extra credit, define `normal_ll` to accept a change of variable. Solution below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cb58f9-abce-4095-b7cf-e89b8f2d2591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def normal_ll(y, y_hat, transform=None, gradient=1):\n",
    "    '''Log likelihood for the normal distribution with change of variable\n",
    "    \n",
    "    The normal distribution is the formal likelihood for the mean squared error (MSE).\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : array_like\n",
    "        Observations.\n",
    "    y_hat : array_like\n",
    "        Predictions.\n",
    "    transform : function\n",
    "        Change of variable transformation.\n",
    "    gradient : function\n",
    "        Gradient of the transform function.\n",
    "        \n",
    "    Proof\n",
    "    -----\n",
    "    https://www.statlect.com/probability-distributions/normal-distribution\n",
    "    '''\n",
    "    if transform is not None:\n",
    "        y = transform(y)\n",
    "        y_hat = transform(y_hat)\n",
    "        \n",
    "    e = y - y_hat\n",
    "    n = len(e)\n",
    "    sigma = e.std()\n",
    "    log_gradient = np.sum(np.log(np.abs(gradient)))\n",
    "    ll = -n * np.log(sigma) - n/2*np.log(2*np.pi) - 1/(2*sigma**2) * (e**2).sum() + log_gradient\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52ef05-83b2-4cb3-9641-2978bd6a8484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93401ea7-e415-4ff0-9c3b-edda956901cd",
   "metadata": {},
   "outputs": [],
   "source": [
    " def compute_weights(series, base=np.e):\n",
    "    '''Compute posterior weights\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series : array_like\n",
    "        Log likelihoods\n",
    "    base: float\n",
    "        Base of the logarithm used to compute log likelihood\n",
    "    '''\n",
    "    s = base**series\n",
    "    return s/s.sum()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
