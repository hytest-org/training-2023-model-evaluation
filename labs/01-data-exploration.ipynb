{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73922d3-1fef-428e-bf82-07e59eaa6f9f",
   "metadata": {},
   "source": [
    "# Lesson: Data Exploration\n",
    "\n",
    "## About \n",
    "This notebook shows a user how to load data using the HyTEST `intake` catalog and `dask`, explore that data using `xarray`, and plot that data using `hvplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c732b-a522-4c43-871d-c0fe8ae3b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import intake\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ecc4f-bb1c-4250-833d-2e38fda0a170",
   "metadata": {
    "tags": []
   },
   "source": [
    "## using `intake`\n",
    "The HyTEST catalog is structured to be compatible with the Python `intake` [package](https://intake.readthedocs.io/en/latest/index.html) and facilitates reading the data into this notebook and others in this training course. The intake catalog is stored as a yaml file, which is easy to parse using other programming languages (even if there is no equivalent to the intake package in that programming language). For an in-depth tutorial, please see the [Pangeo intake tutorial](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html). The intake catalog serves a temporary purpose in our HyTEST repository, and we hope this can be replaced with SpatioTemporal Asset Catalogs [(STAC)](https://www.youtube.com/watch?v=Ugazf5bWsGE) in the near future. To read more about the HyTEST intake catalogs, please view the [hytest repo](https://github.com/hytest-org/hytest/tree/main/dataset_catalog).\n",
    "\n",
    "##### Since we are working on the cloud, we will open a cloud native dataset using `intake`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605355c2-2b0e-418f-863e-cf174d74c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(r\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "\n",
    "# list all the datasets in the catalog\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd235ca-1cfa-4fc5-9132-648daee6386f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see some acronyms of modeling applications (i.e., 'nwm', 'nhm', 'conus404') appended with 'cloud' or 'onprem'; this designates the storage location of the data. To view the full filepaths and URLs behind each data source, please see the yaml file on the [hytest repo](https://github.com/hytest-org/hytest/blob/main/dataset_catalog/hytest_intake_catalog.yml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad121ca-08c8-4e32-aad1-41a04f5bc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a dataset from the above list\n",
    "dataset = \"nwm21-streamflow-usgs-gages-cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778fee-0d0a-49e4-aafc-8a89eaf4bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and view the metadata\n",
    "hytest_cat[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b71bd3-927c-4377-82f8-5b956ac6527f",
   "metadata": {},
   "source": [
    "In some cases, `requester_pays` will be set to `true`. If so, you will need to setup your AWS (Amazon Web Services) credentials to load the data from S3 object storage. Please see this [notebook](https://github.com/hytest-org/hytest/blob/main/environment_set_up/Help_AWS_Credentials.ipynb) for assistance. The good news is our request_pays is set to `false` for this particular dataset so we can continue without an AWS crediential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315c5391-1024-418b-b529-c9ffe899152f",
   "metadata": {},
   "source": [
    "##### Let's say your data is not in the catalog?\n",
    "\n",
    "We could add data from a directory on your local machine, HPC, or within your Cloud file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167d9736-5edc-49d2-bf0c-56b248b98c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c138e34-9c26-476d-a25e-54354415f48d",
   "metadata": {},
   "source": [
    "We can also access data from s3 object storage. Let's try streamflow from the [National Water Model](https://registry.opendata.aws/nwm-archive/) v2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c8bc6-3d4a-4de1-b920-82d86a0a5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load fsspec package; fsspec handles the file access to S3.\n",
    "import fsspec \n",
    "\n",
    "# identify s3 url\n",
    "url = \"s3://noaa-nwm-retro-v2-zarr-pds/\"\n",
    "\n",
    "# generate pseudo file system with fsspec\n",
    "#fs = fsspec.filesystem('s3')\n",
    "#mapper = fs.get_mapper(url)\n",
    "#ds = xr.open_dataset(mapper, engine = 'zarr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a62f07-1dda-4b7b-917d-1cd8b2b7ee9f",
   "metadata": {},
   "source": [
    "## using `dask`\n",
    "\n",
    "To load this data, we will start a parallel cluster using the Python package `dask`, in depth tutorial [here](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/dask.html). Dask parallelism makes use of 'clusters' of workers, each of which is given some task to do. Cluster configurations vary widely, depending on the task and the hardware available. Dask is extremely useful when loading large amounts of data into the notebook and speeds up data loading significantly, especially when accessing data from the cloud. For tutorial on `dask` bag, see [here](https://github.com/hytest-org/hytest/blob/main/essential_reading/Parallel_Dask.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eaf82a-a0c7-4d03-aeba-a030b171e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa05afb-3804-4f9c-9171-a2a3103cc204",
   "metadata": {},
   "source": [
    "users need to set up AWS credentials prior to initializing a cluster because the workers need access to writing abilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747ff1-cac6-4d13-b563-e51092840b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dask_gateway import Gateway\n",
    "except ImportError:\n",
    "    logging.error(\n",
    "        \"Unable to import Dask Gateway.  Are you running in a cloud compute environment?\\n\"\n",
    "    )\n",
    "    raise\n",
    "os.environ[\"DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION\"] = \"1.0\"\n",
    "\n",
    "gateway = Gateway()\n",
    "_options = gateway.cluster_options()\n",
    "_options.conda_environment = (\n",
    "    \"users/users-pangeo\"  ##<< this is the conda environment we use on nebari.\n",
    ")\n",
    "_options.profile = \"Medium Worker\"\n",
    "_env_to_add = {}\n",
    "aws_env_vars = [\n",
    "    \"AWS_ACCESS_KEY_ID\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\",\n",
    "    \"AWS_SESSION_TOKEN\",\n",
    "    \"AWS_DEFAULT_REGION\",\n",
    "]\n",
    "for _e in aws_env_vars:\n",
    "    if _e in os.environ:\n",
    "        _env_to_add[_e] = os.environ[_e]\n",
    "_options.environment_vars = _env_to_add\n",
    "cluster = gateway.new_cluster(_options)  ##<< create cluster via the dask gateway\n",
    "cluster.adapt(minimum=2, maximum=30)  ##<< Sets scaling parameters.\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(\n",
    "    \"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\"\n",
    ")\n",
    "print(\n",
    "    \"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \"\n",
    ")\n",
    "print(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9207d-063e-463c-974f-8d7169bc8502",
   "metadata": {},
   "source": [
    "Note: HyTEST has helper scripts to assist with [cluster initialization](https://github.com/hytest-org/hytest/tree/main/environment_set_up) and a user can run a command like `%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb` when running the notebooks in that repo. See other ipynb files regarding 'Start_Dask_Cluster...ipynb'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618f8a5-7dbf-44bb-8b84-435a18d30cc9",
   "metadata": {},
   "source": [
    "### load dataset with `dask` and `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537df04-b656-4386-adee-a95af8024f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = hytest_cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfb255-871d-4457-a043-eb5a7f4f6f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's view this dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14fd4-7d0a-4354-a569-c4f494db7b5c",
   "metadata": {},
   "source": [
    "From examining the xarray dataset above, we have dimensions of 7994 gage_ids and 367,439 time slices. So what is the timestep? You can use the three disk symbol near the `time` coordinate to examine the values or you can call them out explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d9d90-644e-4822-9dd8-9257ec0f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.time\n",
    "#ds['time.month']\n",
    "#ds['time.year']\n",
    "#ds[season]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9827b8f-77ba-4363-bb59-c1dc82329eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sel year\n",
    "#ds.sel(time=ds.time.dt.year.isin([2005]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0465d71-8165-4600-80c8-aa6496e9cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## censoring data, replacing 0 with 0.001 etc. \n",
    "## checking Nan, Infs check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aed9b1-c6c6-4d33-be09-c7b1c8674791",
   "metadata": {},
   "source": [
    "We see that our timesteps are hourly, and that in our metadata we lack any information with regards to timezone. This is a good example of why its important to contain metadata from your source data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a6c239-f41e-48c3-b986-84a87f6daee6",
   "metadata": {},
   "source": [
    "In the dataset, we also have several data variables (streamflow and velocity), along with coordinates of elevation, gage_id, latitude, longitude, and stream order. The dimensions of the streamflow and velocity variables are time and gage_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21dba9-9169-4eb7-b0c4-f9189bc0ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the first gage? Indexing-style.\n",
    "#ds.gage_id[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54b70f-fd5e-43c1-bdbd-ecdced3c3490",
   "metadata": {},
   "source": [
    "What's the elevation of the first gage? Elevation is a coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8427f9fd-8dea-4d50-8e44-0631702945e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.gage_id[0].elevation.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8efbf5-d2c1-4f6f-8014-c858d81b7e04",
   "metadata": {},
   "source": [
    "##### Question for user: What's the stream order of the first gage? Order is a coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d02fe6-5d01-4030-a6b7-19d41ec55e55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### fill in your thoughts here! #####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bfefe9-68f1-4398-a7d7-516ef9ee533c",
   "metadata": {},
   "source": [
    "Solution! below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b939a-3520-4b1e-be7b-6406b5491a76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f44214-9dc8-46d0-9f8b-c9f43afb2be5",
   "metadata": {},
   "source": [
    "Let's use `dask` to average streamflow for the first gage in our dataset (01030350)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eb97ee-0c84-41d8-a940-ec6a1eedecba",
   "metadata": {},
   "source": [
    "Use 'sel' to find first gage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f87163-dbea-4c59-87cb-4d44de619ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds0 = ds['streamflow'].sel(gage_id = 'USGS-01030350').mean('time')\n",
    "ds0.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e9ef4-fe00-4359-bf2b-aecc82884d75",
   "metadata": {},
   "source": [
    "Let's use `dask` to average streamflow and velocity for EACH GAGE in the dataset (n = 7994). Then we can view the workers performing tasks in real-time using the link that was initialized and supplied to us when we set up our cluster. \n",
    "\n",
    "The task stream is a view of which tasks have been running on each thread of each worker. Each row visible in the task stream subwindow is a thread, and each rectangle represents an individual task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbeee5-c256-4a34-8bd8-96d0b2fdae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.plot style first\n",
    "# hvplot style next, just show one gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48103d8f-6330-4deb-88ec-820c5b9e66ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60cc9e-2d01-4065-b3a6-ba3246f5f25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a9d21-8e02-4a58-b3c5-3a708f1b12a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the groupby function to calculate a descriptive statistic per gage (streamflow gage)\n",
    "ds1 = ds.mean(\"time\")\n",
    "ds1.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9b23c-631c-472c-8d95-7641c9b8e4c6",
   "metadata": {},
   "source": [
    "We now have one mean streamflow and velocity value for each gage in the dataset! But what if we only wanted to average from the year 2000 to 2005? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf066f9-c27b-445d-9144-5de64faac85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds.sel(time=slice('2000-01-01 00:00','2005-12-31 00:00'))\n",
    "ds3 = ds2.mean(\"time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f3923-ce33-47a5-9741-bbeb4690d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "ds3['logQ'] = da.log10(ds3.streamflow)\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf16f3e-0d06-49a8-8723-51757ccec784",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds3.dask.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c4e7d-b757-41ce-871b-f0b70aabaa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3.streamflow.values.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43c0f1-639b-443f-ab4a-af0fe4760414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3.logQ.hvplot.hist(bins = 50)\n",
    "#ds.hvplot.hist(y=streamflow,bins = 50, rasterize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e107a125-b3fc-46e8-bbd0-6786194589e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask bag intro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b907d8-310a-4609-8e16-e86778c884b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823d9d66-c384-4535-b3f1-60ec06429bf8",
   "metadata": {},
   "source": [
    "## using `hvplot`, plot streamflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1999935-1b89-4344-90c0-36777e42b5b4",
   "metadata": {},
   "source": [
    "We will see more with regards to the `hvplot` Python package and its capabilities in the next segment of the tutorial, but for now we wanted to show how one might plot a histogram and hydrograph from a national model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d76603-ff14-4ccb-8898-d16dc1ee8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504765b0-9f61-4e76-a71c-3521856c4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly timeseries\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b15187-8f47-4f84-b934-1e02a9715e61",
   "metadata": {},
   "source": [
    "Let's load our streamflow into memory, for tutorial purposes we will use five years of data per gage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ef62e-2e5f-46b1-8472-6c9538a43769",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds.sel(gage_id='USGS-01030350', time=slice('2000-01-01 00:00','2005-12-31 00:00'))\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585ead-6fed-46b3-b7ba-09aeaa8de98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2.streamflow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3ba60-eafe-4fdc-971b-64c3f09757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "ds2.streamflow.hvplot(x='time', grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6344a-31fb-4a70-b457-cc434adab683",
   "metadata": {},
   "source": [
    "Rasterize = True more than 100 x 200. Good for maps, etc to avoid blowing out memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8329cb1-3bc1-4eca-b8a3-da629d0e488c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361a621-1a9b-47e4-b937-411f46c97a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## maybe add intro to gene's plotting demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd2892-75a3-48cf-aa50-ecd52b4ae616",
   "metadata": {},
   "source": [
    "When working on Cloud, its important to make sure to shutdown all clusters so they can be made available for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2090ac-fefd-4b1c-b9b4-3740d382e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc23816-df4d-4839-81ce-723084dd67fb",
   "metadata": {},
   "source": [
    "The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
