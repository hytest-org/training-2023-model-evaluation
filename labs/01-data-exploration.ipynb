{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73922d3-1fef-428e-bf82-07e59eaa6f9f",
   "metadata": {},
   "source": [
    "# Lesson: Data Exploration\n",
    "\n",
    "## About \n",
    "This notebook shows a user how to load data using the HyTEST `intake` catalog and `dask`, explore that data using `xarray`, and plot that data using `hvplot`.\n",
    "\n",
    "Authors: Sydney Foks, Gene Trantham, Andrew Laws, Tim Hodson, and Rich Signell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67fbb2-022f-40dc-97ff-602b57672c4a",
   "metadata": {},
   "source": [
    "First, we must load some crucial libraries, `intake` and `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c732b-a522-4c43-871d-c0fe8ae3b297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import intake\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ecc4f-bb1c-4250-833d-2e38fda0a170",
   "metadata": {
    "tags": []
   },
   "source": [
    "## using `intake`\n",
    "The HyTEST catalog is structured to be compatible with the Python `intake` [package](https://intake.readthedocs.io/en/latest/index.html) and facilitates reading the data into this notebook (and others in this training course). \n",
    "\n",
    "The intake catalog is stored as a yaml file, which is easy to parse using other programming languages (even if there is no equivalent to the `intake` package in that programming language). For an in-depth tutorial, please see the [Pangeo intake tutorial](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html). Intake is ideal for us in HyTEST because if we change where a dataset gets imported from, we only have to change it in one place (the catalog) rather than in each notebook we reference data. To read more about the HyTEST intake catalogs, please view the [hytest repo](https://github.com/hytest-org/hytest/tree/main/dataset_catalog).\n",
    "\n",
    "##### Channeling our Pangeo concepts, we will open a cloud native dataset using `intake` since we are working in a cloud computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605355c2-2b0e-418f-863e-cf174d74c5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(r\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "\n",
    "# list all the datasets in the catalog\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd235ca-1cfa-4fc5-9132-648daee6386f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see some acronyms of modeling applications (i.e., 'nwm', 'nhm', 'conus404') appended with 'cloud' or 'onprem'; this designates the storage location of the data. To view the full filepaths and URLs behind each data source, please see the yaml file on the [hytest repo](https://github.com/hytest-org/hytest/blob/main/dataset_catalog/hytest_intake_catalog.yml).\n",
    "\n",
    "In the intake catalog, you'll see references to additional catalogs. We call these nested catalogs and they are ideal for housing data with multiple types of calibration schemes or for data that pertains to a course or specific tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0296d1b-c114-4557-bc17-f47f9162ff09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining nested catalogs (example)\n",
    "nested_cat = hytest_cat['nhm-v1.0-daymet-catalog']\n",
    "list(nested_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a95d0f-d33e-4d2b-88fa-5b3ac469d743",
   "metadata": {},
   "source": [
    "For this tutorial we will choose a dataset, the National Water Model version 2.1 which has streamflow but also velocity as we will see in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad121ca-08c8-4e32-aad1-41a04f5bc7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a dataset from the above list\n",
    "dataset = \"nwm21-streamflow-usgs-gages-cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778fee-0d0a-49e4-aafc-8a89eaf4bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and view the metadata\n",
    "hytest_cat[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b71bd3-927c-4377-82f8-5b956ac6527f",
   "metadata": {},
   "source": [
    "In some cases, `requester_pays` will be set to `true`. If so, you will need to setup your AWS (Amazon Web Services) credentials to load the data from S3 object storage. Please see this [notebook](https://github.com/hytest-org/hytest/blob/main/environment_set_up/Help_AWS_Credentials.ipynb) for assistance. The good news is our request_pays is set to `false` for this particular dataset so we can continue without an AWS crediential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a62f07-1dda-4b7b-917d-1cd8b2b7ee9f",
   "metadata": {},
   "source": [
    "## using `dask`\n",
    "\n",
    "To load this data, we will start a parallel cluster using the Python package `dask` (in-depth tutorial [here](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/dask.html)). Dask parallelism makes use of 'clusters' of workers, each of which is given some task to do. Much like inviting your friends to come help you move, having more workers to accomplish a task is ideal and accomplishes the goal quicker. Dask allows for lazy operations, meaning an entire dataset will not be loaded into memory (RAM) until when you want it to be.\n",
    "\n",
    "Cluster configurations vary widely, depending on the task and the hardware available on the compute platform you are using. Dask is extremely useful when loading large amounts of data into the notebook and speeds up data loading significantly, especially when accessing data from the cloud. \n",
    "\n",
    "For tutorial on `dask` bag, see [here](https://github.com/hytest-org/hytest/blob/main/essential_reading/Parallel_Dask.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eaf82a-a0c7-4d03-aeba-a030b171e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa05afb-3804-4f9c-9171-a2a3103cc204",
   "metadata": {},
   "source": [
    "(users need to set up AWS credentials prior to initializing a cluster because the workers need access to writing abilities)\n",
    "\n",
    "The following commands in the cell below are specific to cloud computing, though HyTEST has helper scripts to assist with [cluster initialization](https://github.com/hytest-org/hytest/tree/main/environment_set_up) and a user can run a command `%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb` when running the notebooks in that main [HyTEST repo](https://github.com/hytest-org/hytest). See other ipynb files regarding 'Start_Dask_Cluster...ipynb'.\n",
    "\n",
    "##### initialize cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747ff1-cac6-4d13-b563-e51092840b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from dask_gateway import Gateway\n",
    "except ImportError:\n",
    "    logging.error(\n",
    "        \"Unable to import Dask Gateway.  Are you running in a cloud compute environment?\\n\"\n",
    "    )\n",
    "    raise\n",
    "os.environ[\"DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION\"] = \"1.0\"\n",
    "\n",
    "gateway = Gateway()\n",
    "_options = gateway.cluster_options()\n",
    "_options.conda_environment = (\n",
    "    \"users/users-pangeo\"  ##<< this is the conda environment we use on nebari.\n",
    ")\n",
    "_options.profile = \"Medium Worker\"\n",
    "_env_to_add = {}\n",
    "aws_env_vars = [\n",
    "    \"AWS_ACCESS_KEY_ID\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\",\n",
    "    \"AWS_SESSION_TOKEN\",\n",
    "    \"AWS_DEFAULT_REGION\",\n",
    "]\n",
    "for _e in aws_env_vars:\n",
    "    if _e in os.environ:\n",
    "        _env_to_add[_e] = os.environ[_e]\n",
    "_options.environment_vars = _env_to_add\n",
    "cluster = gateway.new_cluster(_options)  ##<< create cluster via the dask gateway\n",
    "cluster.adapt(minimum=2, maximum=30)  ##<< Sets scaling parameters.\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(\n",
    "    \"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\"\n",
    ")\n",
    "print(\n",
    "    \"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \"\n",
    ")\n",
    "print(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9207d-063e-463c-974f-8d7169bc8502",
   "metadata": {},
   "source": [
    "The above link is important for visualizing the Cluster Map and Task Stream for the cluster that we just initialized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618f8a5-7dbf-44bb-8b84-435a18d30cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset with `dask` and `xarray`\n",
    "We are now going to call our dataset from our intake catalog and load it to dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537df04-b656-4386-adee-a95af8024f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = hytest_cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfb255-871d-4457-a043-eb5a7f4f6f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's view this dataset\n",
    "# type(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14fd4-7d0a-4354-a569-c4f494db7b5c",
   "metadata": {},
   "source": [
    "From examining the xarray dataset above, we have dimensions of 7994 gage_ids and 367,439 time slices. \n",
    "\n",
    "We also have several data variables (streamflow and velocity), along with coordinates of elevation, gage_id, latitude, longitude, and stream order. The dimensions of the streamflow and velocity variables are time and gage_id.\n",
    "\n",
    "So what is the timestep in this dataset? You can use the three disk symbol near the `time` coordinate to examine the values or you can call them out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cf57f-44d3-4759-8dfd-fd03e678a940",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['time']\n",
    "#ds['time.month']\n",
    "#ds['time.year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ed5c8-aeae-4544-bafd-4f1b3f443cc1",
   "metadata": {},
   "source": [
    "We see that our timesteps are hourly, and that in our metadata we lack any information with regards to timezone. This is a good example of why its important to contain metadata from your source data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6514b4-7292-4b12-bffd-9cc705ea0ebb",
   "metadata": {},
   "source": [
    "##### We can use the `sel` functions to select values or character strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d9d90-644e-4822-9dd8-9257ec0f42ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select a year of data for all gages in the dataset\n",
    "ds.sel(time = '2005')\n",
    "\n",
    "## select only streamflow for all gages for only 2005\n",
    "#ds.streamflow.sel(time = '2005')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68671325-db4d-4ff0-84f6-21d2b2110b9f",
   "metadata": {},
   "source": [
    "##### We can use `isel` to select indices (index select) within the array or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20928f51-d8e2-4cb1-b934-222f7b59a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first gage id in the dataset using isel function. \n",
    "ds.isel(gage_id = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911d5ab-67d7-461e-b7e2-2fa52b727fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select streamflow for the first gage id in the dataset using isel function. \n",
    "ds.streamflow.isel(gage_id = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7e816-ca75-4227-91f0-f3aa7b25722f",
   "metadata": {},
   "source": [
    "##### Coordinates that are not directly a dimension of the any of the variables have to be called out explicitly to examine the data. So how do examine latitude/longitude of a gage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9827b8f-77ba-4363-bb59-c1dc82329eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## traditional indexing:\n",
    "# ds.gage_id[0].latitude.values\n",
    "\n",
    "## using isel:\n",
    "ds.isel(gage_id = 1).latitude.values\n",
    "\n",
    "## using sel:\n",
    "#ds.sel(gage_id = \"USGS-01030500\").latitude.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05269b-a320-412d-8b66-1a2c92365ad5",
   "metadata": {},
   "source": [
    "##### Question for user: What's the stream order of the first gage in our dataset? Order is a coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e397b-65ad-44b3-ba3b-721708db3c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12bded-b99d-4f21-8ce6-7560add32346",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# answer is:\n",
    "# ds.isel(gage_id = 1).order.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319e2ff-b992-4355-974b-b1a1be343977",
   "metadata": {},
   "source": [
    "##### Censoring data, checking for NaNs, InFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0465d71-8165-4600-80c8-aa6496e9cda7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71beab57-cc4a-498e-8e7c-bf2e5013346c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2a38f9-3bb0-4e9b-9f42-02988ff7a800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a6c239-f41e-48c3-b986-84a87f6daee6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37f44214-9dc8-46d0-9f8b-c9f43afb2be5",
   "metadata": {},
   "source": [
    "##### Let's use `dask` to average streamflow for the first gage in our dataset (01030350)\n",
    "\n",
    "Use `sel` to find first gage and add `mean` to average over the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f87163-dbea-4c59-87cb-4d44de619ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds0 = ds['streamflow'].sel(gage_id = 'USGS-01030350').mean('time')\n",
    "ds0.compute().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e9ef4-fe00-4359-bf2b-aecc82884d75",
   "metadata": {},
   "source": [
    "##### Let's use `dask` to average streamflow and velocity for the first 100 gages in the dataset (total n = 7994). \n",
    "\n",
    "We can view the workers performing tasks in real-time using the link that was initialized and supplied to us when we set up our cluster. \n",
    "\n",
    "The task stream is a view of which tasks have been running on each thread of each worker. Each row visible in the task stream subwindow is a thread, and each rectangle represents an individual task.\n",
    "The cluster map is showing the data exchange between nodes.\n",
    "\n",
    "This next cell will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97152ce2-5148-4c5b-8a4a-4a9097f68d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds1 = ds.isel(gage_id=slice(0,100)).mean('time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd8bc6-3691-4ed8-a2b4-763ce54207ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9b23c-631c-472c-8d95-7641c9b8e4c6",
   "metadata": {},
   "source": [
    "##### We now have one mean streamflow and velocity value for each of the 100 gages in the dataset! But what if we only wanted an hourly average from the year 2000 to 2005? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf066f9-c27b-445d-9144-5de64faac85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds2 = ds.sel(time=slice('2000-01-01 00:00','2005-12-31 00:00')).isel(gage_id=slice(0,100)).mean(\"time\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8c9e2a-39bb-433b-ab34-66848f90ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4a94c-872d-4bf8-b52a-b5addf108789",
   "metadata": {},
   "source": [
    "##### What if we want annual sums from 2000 to 2005 for the first 100 gages in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e82c2-da5c-4021-8eb6-f6be3b2800bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf349f8-2436-45b8-8df8-eae4f2d8b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b6f15-aedf-4609-903c-eba18edc7a40",
   "metadata": {},
   "source": [
    "##### What about monthly sums? How would the command change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736ba5c-2654-4362-827b-392afc8b2a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaa7997-4301-46e1-a1c2-e4b579577194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c747272-40db-408e-ba61-491e53a3198e",
   "metadata": {},
   "source": [
    "##### Now let's use a bounding box to grab gages of interest, then let's calculate annual sums for five years for each of the gages in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302543-fde6-480d-9bc8-2cf9f5bee5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fafc938-844c-4782-89e7-0beb3e225c4f",
   "metadata": {},
   "source": [
    "##### Let's use a shapefile to find gages of interest ... I wonder if this will bonk if we grab a shapefile from ScienceBase .. maybe just point to conus404 data access notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585278e4-c1fa-471c-ab95-2727f54b3cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823d9d66-c384-4535-b3f1-60ec06429bf8",
   "metadata": {},
   "source": [
    "## using `hvplot`, plot streamflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1999935-1b89-4344-90c0-36777e42b5b4",
   "metadata": {},
   "source": [
    "We will see more with regards to the `hvplot` Python package and its capabilities in the next segment of the tutorial, but for now we wanted to show how one might plot a histogram and hydrograph from a national model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d76603-ff14-4ccb-8898-d16dc1ee8649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import hvplot.xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1f03b-4468-4ed4-8917-6a060b9e3133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.plot style first\n",
    "# hvplot style next, just show one gage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19478428-a9ba-4897-bfcc-3d133bdb4cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "ds3['logQ'] = da.log10(ds3.streamflow)\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cda65f-4b4f-479c-a2f5-f936fb9c5e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3.logQ.hvplot.hist(bins = 50)\n",
    "#ds.hvplot.hist(y=streamflow,bins = 50, rasterize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301265c-30de-4836-af14-30631510afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds3.dask.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504765b0-9f61-4e76-a71c-3521856c4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# monthly timeseries\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b15187-8f47-4f84-b934-1e02a9715e61",
   "metadata": {},
   "source": [
    "Let's load our streamflow into memory, for tutorial purposes we will use five years of data per gage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ef62e-2e5f-46b1-8472-6c9538a43769",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = ds.sel(gage_id='USGS-01030350', time=slice('2000-01-01 00:00','2005-12-31 00:00'))\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4585ead-6fed-46b3-b7ba-09aeaa8de98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds2.streamflow.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f3ba60-eafe-4fdc-971b-64c3f09757cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "ds2.streamflow.hvplot(x='time', grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a6344a-31fb-4a70-b457-cc434adab683",
   "metadata": {},
   "source": [
    "Rasterize = True more than 100 x 200. Good for maps, etc to avoid blowing out memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8329cb1-3bc1-4eca-b8a3-da629d0e488c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8361a621-1a9b-47e4-b937-411f46c97a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fdd2892-75a3-48cf-aa50-ecd52b4ae616",
   "metadata": {},
   "source": [
    "##### When working on Cloud, its important to make sure to shutdown all clusters so they can be made available for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2090ac-fefd-4b1c-b9b4-3740d382e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f4c8e-3da4-4257-8b58-e07c51422ba6",
   "metadata": {},
   "source": [
    "##### Segway into next section:\n",
    "- In this notebook, we covered very basic ways to explore data with `dask`, `xarray`, and `hvplot`\n",
    "- The next notebook, we focus on more advanced plotting with `hvplot` and `panels`, both packages supported by the Pangeo platform.\n",
    "\n",
    "The End. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb8752-24a5-4884-ad4a-ba7d72897ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
