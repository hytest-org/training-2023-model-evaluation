{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f73922d3-1fef-428e-bf82-07e59eaa6f9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lesson: Data Exploration\n",
    "\n",
    "## About \n",
    "This notebook shows a user how to load data using the HyTEST `intake` catalog and `dask`, explore that data using `xarray`, and plot that data using `hvplot`.\n",
    "\n",
    "Authors: Sydney Foks, Gene Trantham, Andrew Laws, Tim Hodson, and Rich Signell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67fbb2-022f-40dc-97ff-602b57672c4a",
   "metadata": {},
   "source": [
    "First, we must load some crucial libraries, `intake` and `xarray`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c732b-a522-4c43-871d-c0fe8ae3b297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import intake\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ecc4f-bb1c-4250-833d-2e38fda0a170",
   "metadata": {
    "tags": []
   },
   "source": [
    "## using `intake`\n",
    "The HyTEST catalog is structured to be compatible with the Python `intake` [package](https://intake.readthedocs.io/en/latest/index.html) and facilitates reading the data into this notebook (and others in this training course). \n",
    "\n",
    "The intake catalog is stored as a yaml file, which is easy to parse using other programming languages (even if there is no equivalent to the `intake` package in that programming language). For an in-depth tutorial, please see the [Pangeo intake tutorial](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/intake.html). Intake is ideal for us in HyTEST because if we change where a dataset gets imported from, we only have to change it in one place (the catalog) rather than in each notebook we reference data. To read more about the HyTEST intake catalogs, please view the [hytest repo](https://github.com/hytest-org/hytest/tree/main/dataset_catalog).\n",
    "\n",
    "##### Channeling our Pangeo concepts, we will open a cloud native dataset using `intake` since we are working in a cloud computing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605355c2-2b0e-418f-863e-cf174d74c5d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# open the hytest data intake catalog\n",
    "hytest_cat = intake.open_catalog(r\"https://raw.githubusercontent.com/hytest-org/hytest/main/dataset_catalog/hytest_intake_catalog.yml\")\n",
    "\n",
    "# list all the datasets in the catalog\n",
    "list(hytest_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd235ca-1cfa-4fc5-9132-648daee6386f",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see some acronyms of modeling applications (i.e., 'nwm', 'nhm', 'conus404') appended with 'cloud' or 'onprem'; this designates the storage location of the data. To view the full filepaths and URLs behind each data source, please see the yaml file on the [hytest repo](https://github.com/hytest-org/hytest/blob/main/dataset_catalog/hytest_intake_catalog.yml).\n",
    "\n",
    "In the intake catalog, you'll see references to additional catalogs. We call these nested catalogs and they are ideal for housing data with multiple types of calibration schemes or for data that pertains to a course or specific tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0296d1b-c114-4557-bc17-f47f9162ff09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examining nested catalogs (example)\n",
    "nested_cat = hytest_cat['nhm-v1.0-daymet-catalog']\n",
    "list(nested_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a95d0f-d33e-4d2b-88fa-5b3ac469d743",
   "metadata": {},
   "source": [
    "For this tutorial we will choose a dataset, the National Water Model version 2.1 which has streamflow but also velocity as we will see in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad121ca-08c8-4e32-aad1-41a04f5bc7f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# choose a dataset from the above list\n",
    "dataset = \"nwm21-streamflow-usgs-gages-cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778fee-0d0a-49e4-aafc-8a89eaf4bc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and view the metadata and more!\n",
    "hytest_cat[dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b71bd3-927c-4377-82f8-5b956ac6527f",
   "metadata": {},
   "source": [
    "In some cases, `requester_pays` will be set to `true`. If so, you will need to setup your AWS (Amazon Web Services) credentials to load the data from S3 object storage. Please see this [notebook](https://github.com/hytest-org/hytest/blob/main/environment_set_up/Help_AWS_Credentials.ipynb) for assistance. The good news is our request_pays is set to `false` for this particular dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a62f07-1dda-4b7b-917d-1cd8b2b7ee9f",
   "metadata": {},
   "source": [
    "## using `dask`\n",
    "\n",
    "To load this data, we will start a parallel cluster using the Python package `dask` (in-depth tutorial [here](http://gallery.pangeo.io/repos/pangeo-data/pangeo-tutorial-gallery/dask.html)). Dask parallelism makes use of 'clusters' of workers, each of which is given some task to do. Much like inviting your friends to come help you move, having more workers to accomplish a task is ideal and accomplishes the goal quicker. Dask allows for lazy operations, meaning an entire dataset will not be loaded into memory (RAM) until when you want it to be.\n",
    "\n",
    "Cluster configurations vary widely, depending on the task and the hardware available on the compute platform you are using. Dask is extremely useful when loading large amounts of data into the notebook and speeds up data loading significantly, especially when accessing data from the cloud. \n",
    "\n",
    "For tutorial on `dask` bag, see [here](https://github.com/hytest-org/hytest/blob/main/essential_reading/Parallel_Dask.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eaf82a-a0c7-4d03-aeba-a030b171e79b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa05afb-3804-4f9c-9171-a2a3103cc204",
   "metadata": {},
   "source": [
    "(users need to set up AWS credentials prior to initializing a cluster because the workers need access to writing abilities)\n",
    "\n",
    "The following commands in the cell below are specific to cloud computing, though HyTEST has helper scripts to assist with [cluster initialization](https://github.com/hytest-org/hytest/tree/main/environment_set_up) and a user can run a command `%run ../environment_set_up/Start_Dask_Cluster_Nebari.ipynb` when running the notebooks in that main [HyTEST repo](https://github.com/hytest-org/hytest). See other ipynb files regarding 'Start_Dask_Cluster...ipynb'.\n",
    "\n",
    "##### initialize cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69747ff1-cac6-4d13-b563-e51092840b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from dask_gateway import Gateway\n",
    "except ImportError:\n",
    "    logging.error(\n",
    "        \"Unable to import Dask Gateway.  Are you running in a cloud compute environment?\\n\"\n",
    "    )\n",
    "    raise\n",
    "os.environ[\"DASK_DISTRIBUTED__SCHEDULER__WORKER_SATURATION\"] = \"1.0\"\n",
    "\n",
    "gateway = Gateway()\n",
    "_options = gateway.cluster_options()\n",
    "_options.conda_environment = (\n",
    "    \"users/users-pangeo\"  ##<< this is the conda environment we use on nebari.\n",
    ")\n",
    "_options.profile = \"Medium Worker\"\n",
    "_env_to_add = {}\n",
    "aws_env_vars = [\n",
    "    \"AWS_ACCESS_KEY_ID\",\n",
    "    \"AWS_SECRET_ACCESS_KEY\",\n",
    "    \"AWS_SESSION_TOKEN\",\n",
    "    \"AWS_DEFAULT_REGION\",\n",
    "]\n",
    "for _e in aws_env_vars:\n",
    "    if _e in os.environ:\n",
    "        _env_to_add[_e] = os.environ[_e]\n",
    "_options.environment_vars = _env_to_add\n",
    "cluster = gateway.new_cluster(_options)  ##<< create cluster via the dask gateway\n",
    "cluster.adapt(minimum=2, maximum=30)  ##<< Sets scaling parameters.\n",
    "\n",
    "client = cluster.get_client()\n",
    "\n",
    "print(\n",
    "    \"The 'cluster' object can be used to adjust cluster behavior.  i.e. 'cluster.adapt(minimum=10)'\"\n",
    ")\n",
    "print(\n",
    "    \"The 'client' object can be used to directly interact with the cluster.  i.e. 'client.submit(func)' \"\n",
    ")\n",
    "print(f\"The link to view the client dashboard is:\\n>  {client.dashboard_link}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e9207d-063e-463c-974f-8d7169bc8502",
   "metadata": {},
   "source": [
    "The above link is important for visualizing the Cluster Map and Task Stream for the cluster that we just initialized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618f8a5-7dbf-44bb-8b84-435a18d30cc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load dataset with `dask` and `xarray`\n",
    "We are now going to call our dataset from our intake catalog and load it to dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537df04-b656-4386-adee-a95af8024f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = hytest_cat[dataset].to_dask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccfb255-871d-4457-a043-eb5a7f4f6f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's view this dataset\n",
    "# type(ds)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a14fd4-7d0a-4354-a569-c4f494db7b5c",
   "metadata": {},
   "source": [
    "From examining the xarray dataset above, we have dimensions of 7994 gage_ids and 367,439 time slices. \n",
    "\n",
    "We also have several data variables (streamflow and velocity), along with coordinates of elevation, gage_id, latitude, longitude, and stream order. The dimensions of the streamflow and velocity variables are time and gage_id.\n",
    "\n",
    "So what is the timestep in this dataset? You can use the three disk symbol near the `time` coordinate to examine the values or you can call them out explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cf57f-44d3-4759-8dfd-fd03e678a940",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds['time']\n",
    "#ds['time.month']\n",
    "#ds['time.year']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ed5c8-aeae-4544-bafd-4f1b3f443cc1",
   "metadata": {},
   "source": [
    "We see that our timesteps are hourly, and that in our metadata we lack any information with regards to timezone. This is a good example of why its important to contain metadata from your source data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6514b4-7292-4b12-bffd-9cc705ea0ebb",
   "metadata": {},
   "source": [
    "##### We can use the `sel` functions to select values or character strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39d9d90-644e-4822-9dd8-9257ec0f42ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select a year of data for all gages in the dataset\n",
    "ds.sel(time = '2005')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0af404-d682-4c2e-a910-3ea5d7160b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select only streamflow for all gages for only 2005\n",
    "ds.streamflow.sel(time = '2005')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68671325-db4d-4ff0-84f6-21d2b2110b9f",
   "metadata": {},
   "source": [
    "##### We can use `isel` to select indices (index select) within the array or matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20928f51-d8e2-4cb1-b934-222f7b59a98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select first gage id in the dataset using isel function. \n",
    "ds.isel(gage_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911d5ab-67d7-461e-b7e2-2fa52b727fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select streamflow for the first gage id in the dataset using isel function. \n",
    "ds.streamflow.isel(gage_id = 19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e7e816-ca75-4227-91f0-f3aa7b25722f",
   "metadata": {},
   "source": [
    "##### Coordinates that are not directly a dimension of the any of the variables have to be called out explicitly to examine the data. So how do examine latitude/longitude of a gage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9827b8f-77ba-4363-bb59-c1dc82329eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## traditional indexing:\n",
    "# ds.gage_id[0].latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5a87cf-957f-48ca-8ab5-f1bd78e5ae9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de73605-4aa2-477b-bb11-70293c9ab5c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using isel:\n",
    "ds.isel(gage_id = 1).latitude.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351bfdec-4802-4a72-9baa-38af79bec38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using sel:\n",
    "ds.sel(gage_id = \"USGS-01030500\").latitude.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05269b-a320-412d-8b66-1a2c92365ad5",
   "metadata": {},
   "source": [
    "##### Question for user: What's the stream order of the first gage in our dataset? Order is a coordinate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34e397b-65ad-44b3-ba3b-721708db3c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in your thoughts here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12bded-b99d-4f21-8ce6-7560add32346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# answer is:\n",
    "ds.isel(gage_id = 1).order.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319e2ff-b992-4355-974b-b1a1be343977",
   "metadata": {},
   "source": [
    "##### Data Checks: checking for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0465d71-8165-4600-80c8-aa6496e9cda7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616ecc25-71b1-443d-834e-d97818a20145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Are there any NaNs for the first 25 gages?\n",
    "ds0 = ds.isel(gage_id=slice(0,25))\n",
    "nans = da.isnan(ds0).any().compute()\n",
    "nans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f44214-9dc8-46d0-9f8b-c9f43afb2be5",
   "metadata": {},
   "source": [
    "##### Let's use `dask` to average streamflow for the first gage in our dataset (01030350)\n",
    "\n",
    "Use `sel` to find first gage and add `mean` to average over the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f87163-dbea-4c59-87cb-4d44de619ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds0 = ds['streamflow'].sel(gage_id = 'USGS-01030350').mean('time')\n",
    "ds0.compute().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e9ef4-fe00-4359-bf2b-aecc82884d75",
   "metadata": {},
   "source": [
    "##### Let's use `dask` to average streamflow and velocity for the first 100 gages in the dataset (total n = 7994). \n",
    "\n",
    "We can view the workers performing tasks in real-time using the link that was initialized and supplied to us when we set up our cluster. \n",
    "\n",
    "The task stream is a view of which tasks have been running on each thread of each worker. Each row visible in the task stream subwindow is a thread, and each rectangle represents an individual task.\n",
    "The cluster map is showing the data exchange between nodes.\n",
    "\n",
    "This next cell will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97152ce2-5148-4c5b-8a4a-4a9097f68d43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds.isel(gage_id=slice(0,99)).mean('time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4f591-50aa-4d1c-b80a-9fbe993cc176",
   "metadata": {},
   "source": [
    "##### We can also use `dask` with `hvplot` as well\n",
    "\n",
    "We will use the `hvplot` Python package to create an interactive hydrograph for the first gage from 2000-2005. We will see more with regards to the `hvplot` and its capabilities in the next segment of the tutorial, but for now this is just a sneak preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79255b84-ccf8-42fa-bca9-3fafae889e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load our streamflow into memory for our first gage, we will use five years of data per gage for the tutorial.\n",
    "ds2 = ds.sel(gage_id='USGS-01030350', time=slice('2000-01-01 00:00','2005-12-31 00:00'))\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db354e72-ba2c-4d79-bf24-10f3dca8e9c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hvplot.xarray\n",
    "ds2.streamflow.hvplot(x='time', grid = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9b23c-631c-472c-8d95-7641c9b8e4c6",
   "metadata": {},
   "source": [
    "##### What if we wanted to take the hourly mean for 100 gages in the dataset from the year 2000 to 2005? What does this look like with dask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf066f9-c27b-445d-9144-5de64faac85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds2 = ds.sel(time=slice('2000-01-01 00:00','2005-12-31 00:00')).isel(gage_id=slice(0,99)).mean(\"time\").compute()\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac55d3-342e-4eaa-9e94-e4103cc07a2d",
   "metadata": {},
   "source": [
    "##### How do we average our hourly data to daily averages\n",
    "`resample` function is helpful for this application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba81aa-f9f1-4269-8ac0-19299f67148d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds2 = ds.isel(gage_id = slice(0,49)).resample(time='1D').mean('time', keep_attrs=True).compute()\n",
    "ds2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b4a94c-872d-4bf8-b52a-b5addf108789",
   "metadata": {},
   "source": [
    "##### What if we want annual daily maximums from 2000 to 2005 for the first 50 gages in the dataset?\n",
    "We can build off ds2 that we generated in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e82c2-da5c-4021-8eb6-f6be3b2800bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in thoughts here. answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf349f8-2436-45b8-8df8-eae4f2d8b1e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "ds2.sel(time=slice('2000-01-01 00:00','2005-12-31 00:00')).resample(time='1AS').max('time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b6f15-aedf-4609-903c-eba18edc7a40",
   "metadata": {},
   "source": [
    "##### What about monthly averages from 2000-2005? How would the command change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736ba5c-2654-4362-827b-392afc8b2a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in thoughts here. answer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83549d-ef1e-4746-9236-adedaebd7ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Answer:\n",
    "ds2.sel(time=slice('2000-01-01 00:00','2005-12-31 00:00')).resample(time='1M').mean('time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd161fc-fcbb-4ec1-b4c5-9f8a80f5335c",
   "metadata": {},
   "source": [
    "##### Could we add variables? plot histograms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57054cd-e93d-454c-88f5-38ba55c0a6be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "ds2['logQ'] = da.log10(ds2.streamflow)\n",
    "ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15f466-a094-4479-8575-1b4b19331629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds2.logQ.hvplot.hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c747272-40db-408e-ba61-491e53a3198e",
   "metadata": {},
   "source": [
    "##### How would we use a bounding box to grab gages of interest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1f93c-55f7-4ed7-a478-0f81d78ca5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d0beab-fe97-47e2-8460-445646cc1b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d302543-fde6-480d-9bc8-2cf9f5bee5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# identify bounding box (Colorado as example)\n",
    "latmin = 36.992\n",
    "latmax = 41.003\n",
    "longmin = -109.060\n",
    "longmax = -102.042\n",
    "\n",
    "# latitude and longitude are not dimensions of our data so we have to call their values to subset\n",
    "lats = ds['latitude'].values\n",
    "longs = ds['longitude'].values\n",
    "\n",
    "# if the lat/longs fall within the range in the bounding box\n",
    "latid = np.where((lats >= latmin) & (lats <= latmax))[0]\n",
    "longid = np.where((longs >= longmin) & (longs <= longmax))[0]\n",
    "\n",
    "# if the indices match, identify it as a gage of interest, mark the index location\n",
    "inds = np.intersect1d(latid,longid)\n",
    "\n",
    "# extract the indices out using isel\n",
    "ds1 = ds.isel(gage_id = inds)\n",
    "ds1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdd2892-75a3-48cf-aa50-ecd52b4ae616",
   "metadata": {},
   "source": [
    "##### When working on Cloud, its important to make sure to shutdown all clusters so they can be made available for others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2090ac-fefd-4b1c-b9b4-3740d382e574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f4c8e-3da4-4257-8b58-e07c51422ba6",
   "metadata": {},
   "source": [
    "##### Segway into next section:\n",
    "- In this notebook, we covered very basic ways to explore data with `dask`, `xarray`, and `hvplot`\n",
    "- The next notebook, we focus on more advanced plotting with `hvplot` and `panels`, both packages supported by the Pangeo platform.\n",
    "\n",
    "The End. Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "users-users-pangeo",
   "language": "python",
   "name": "conda-env-users-users-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
